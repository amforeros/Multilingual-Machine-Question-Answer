{"cells":[{"cell_type":"markdown","metadata":{"id":"aFl39Ejxoq6c"},"source":["# Week 39 - Sequence Supervised Classifier"]},{"cell_type":"markdown","metadata":{"id":"0jDRMwJFG31h"},"source":["## 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"S5n302rlIblM"},"source":["### 1.1. Libraries"]},{"cell_type":"markdown","metadata":{"id":"wjs9ApFwMrog"},"source":["#### 1.1.1. New Libraries"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40155,"status":"ok","timestamp":1698849622629,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"z08V_ZPyIecS","outputId":"7d698ea3-8b2c-41f1-f65e-2feb58d8b1a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.10.12\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Requirement already satisfied: datasets==2.2.1 in /usr/local/lib/python3.10/dist-packages (2.2.1)\n","Requirement already satisfied: transformers==4.19.1 in /usr/local/lib/python3.10/dist-packages (4.19.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (1.23.5)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (9.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (0.18.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (23.2)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets==2.2.1) (0.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1) (3.12.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1) (2023.6.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1) (0.12.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.2.1) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.2.1) (3.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.2.1) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.2.1) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.2.1) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.2.1) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.2.1) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.2.1) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.2.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.2.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.2.1) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.2.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.2.1) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.2.1) (1.16.0)\n","Requirement already satisfied: bnlp-toolkit in /usr/local/lib/python3.10/dist-packages (4.0.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (0.1.99)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (4.3.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (1.11.3)\n","Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (0.3.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (4.66.1)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (6.1.1)\n","Requirement already satisfied: emoji==1.7.0 in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (1.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (2.31.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->bnlp-toolkit) (0.2.8)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bnlp-toolkit) (6.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit) (2023.6.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (2023.7.22)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit) (0.9.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit) (1.16.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit) (0.9.0)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.19.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.12.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->transformers[torch]) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->transformers[torch]) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->transformers[torch]) (1.3.0)\n","Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.4)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.1)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2023.7.22)\n","Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: evaluate\n","Successfully installed evaluate-0.4.1\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=6f349f7c87a6b3b9bda1ca50796d511e210196e82b7fb2f6c2d12ebb3eb801d4\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}],"source":["!python --version\n","!pip3 install nltk                                      # new libraries\n","!pip install datasets==2.2.1 transformers==4.19.1\n","!pip3 install bnlp-toolkit                              # Bengali_Tokenization\n","!pip3 install transformers[torch]                       # hyperparameters\n","!pip3 install bpemb                                     # pretrain word embeddings\n","!pip install evaluate                                   # evaluate model huggingface\n","!pip install seqeval                                    # token classification metric"]},{"cell_type":"markdown","metadata":{"id":"VkrY-lgQMv-J"},"source":["#### 1.1.2. Load Libraries"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2013,"status":"ok","timestamp":1698849920402,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"dZ2gX_uzuaOa","outputId":"bd937d46-0e20-4251-a5bd-4028e3661dc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from datasets import load_dataset                                                    # library to import data from huggingface\n","import warnings                                                                      # ignore warnings in printing\n","warnings.filterwarnings(\"ignore\")\n","from torch import nn                                                                 # neural networks\n","import torch                                                                         # torch for managing special python objects\n","import numpy as np                                                                   # library for math operations and matrices\n","import random                                                                        # library for replicating results\n","from typing import List, Tuple                                                       # library format functionsin dataloader and torch objects\n","from torch.utils.data import Dataset, DataLoader                                     # library dataloader and dataset in training nn\n","import heapq                                                                         # beam searching for finding the most likely sequence\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix        # evaluation metrics\n","from torch.optim import Adam                                                         # optimizer\n","from tqdm.notebook import tqdm                                                       # print progress loop\n","from google.colab import drive                                                       # google colab\n","drive.mount('/content/drive')\n","from sklearn.metrics import f1_score, accuracy_score                                 # f1_score, accuracy\n","from transformers import AdamW, AutoTokenizer                                        # transformer: optimizer, tokenizer (pre-train model), model\n","from transformers import AutoModelForTokenClassification                             # Token Classification\n","from transformers import DataCollatorForTokenClassification                          # Data Collator for token classification\n","from transformers import TrainingArguments                                           # Hyperparametes\n","from transformers import Trainer                                                     # Trainer\n","import evaluate\n","from datasets import load_metric                                                     # Evaluation metric\n","from functools import partial\n","\n","#import pandas as pd                                                                  # library to transform to dataframe. helps for statistics\n","#from bpemb import BPEmb                                                              # embeddings\n","#from sklearn.linear_model import LogisticRegression                                  # model\n","#from sklearn.metrics import classification_report                                    # classification report binary clasiffier\n","#from sklearn.metrics import recall_score, precision_score                            # recall_score, precision_score\n","#from torch.utils.data import Dataset, DataLoader                                     # torch for managing special data types\n","#from typing import List, Tuple                                                       # data structures in outputs\n","#from tqdm.notebook import tqdm                                                       # show progress of the loop\n","#\n","#from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification    # transformer: optimizer, tokenizer (pre-train model), model\n","#from transformers import BertForSequenceClassification                               # Load trained model\n","#from transformers import DataCollatorWithPadding                                     # for padding in batches\n","#from datasets import load_metric                                                     # Evaluation metric\n","#"]},{"cell_type":"markdown","metadata":{"id":"zrOaTX15Ie8s"},"source":["### 1.2. Data"]},{"cell_type":"markdown","metadata":{"id":"xQkFMJpJKTij"},"source":["#### 1.2.1. Read Data"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1698849629412,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"co_f3SN1J8FN","outputId":"99c13175-a435-44b9-ea34-d3688b185f97"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.builder:Using custom data configuration copenlu--nlp_course_tydiqa-42333912ea665dd0\n","WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-42333912ea665dd0/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","WARNING:datasets.builder:Using custom data configuration copenlu--nlp_course_tydiqa-42333912ea665dd0\n","WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-42333912ea665dd0/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"]}],"source":["# define languages for the project\n","languages = ['arabic', 'bengali','indonesian']\n","\n","# load training dataset\n","datasets_train = load_dataset(\"copenlu/answerable_tydiqa\", split='train')\n","# load validation dataset\n","datasets_val = load_dataset(\"copenlu/answerable_tydiqa\", split='validation')\n","# set gpu if available\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","def enforce_reproducibility(seed=42):\n","    # Sets seed manually for both CPU and CUDA\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # For atomic operations there is currently\n","    # no simple way to enforce determinism, as\n","    # the order of parallel operations is not known.\n","    # CUDNN\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # System based\n","    random.seed(seed)\n","    np.random.seed(seed)\n","enforce_reproducibility()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246,"referenced_widgets":["31892793b9934bf1a41050f81978c5d9","30e22b5a9a104f098924c98d05f66951","1f7a60272038453692009f0448075011","e9c63e9490d74a6f909219deedb209a7","85da4d4e5071433caec15e22c0711d62","a23b21e1a56c4813a7634542c34572b2","a80b8d64dc9a40278813b459da10151e","af769f5f607d4d7795ac92933309cfc5","60d0693f7c3c483e947dfcbc1c0e705a","b14e55db5a084958a38dfc17d15f492f","514b28d7e30346949acd781669236dd6","fff5ae40b37d4a83a2764f57533e73c8","2657b8ab0f1541ab8ea09e6e938deadd","a4b1bf184d204d0bbbf51e2f8ac600cc","e28bdc359b3e4b31a9addaff54007154","73889590018048f3801e0ce3b1cf5375","7227e32af5514a19b1642bbb5218e3e7","a01bd39a65f24c5bbea951c9df98eca8","395bb72789c146678a5a2afaead4feac","c32332b0352a40eca45820bfacb52d74","b11d21b1220740ee81b7bb7ade490760","fcbe0154575547b896b15f832d3c6ab7","53809765cd604b0f9e11ab501cf8ac88","2539fee6f6d44175aa8b4605d7767c39","a705d8ad88a64e2fa5a28e2484f978fe","86d471fcb7ac4b8dacceee060be7e010","b3ec58ddc77d4dc9964d356a91488d20","61f433706f9b44478cbd986669bf6905","a798672228d74e56b6fa92a1c1270a42","e3c5f9dfd76d497a84f5331d21c92918","4d87f24749544dc8a4487afd7c1b6740","6e60e27b5dbf4759a7ee84e8687eab2f","89c7e07297864eb38308186a9f06f31d","da8cdd9f78b949ecaed2ff6db2ed4c0f","b1e0efcffede4fa4b3594412bb22d94f","1966ff0ac1534573a071491ff0f0d6a4","b78cc6d4711c4720bd96655f39b16585","bd431f87df954ebb9b89943f4f8f7d9a","5d74bae52486476fb832cabd8a2c4d23","15ce8c7865144452a6c0d72e4d999d60","f71cd8183aa743f0b3aebd74b35a0ff5","003d8598eb9e4b43af27eb77c3802b34","6620619e1717491792d8feb0cb59f2fe","4592ef950a0a4aaabd9c196fe3027716","6de100507b1b4f39871e87a2f08ca2a5","40b8343768244ba1932cff7c8d2a1872","93a69455f1b945d599fc4387c30b5ff1","c1ca56c7f4f349aa9fa3efb59c9fbc5f","d75a8dfd0d914d0a8d5ad6ed906e82b4","6890df1add7a4cb9bc5d24f6dca7fd44","50ea2de0c13b47eb87417332c5e89983","1a009319c8914d2282ea96bac19dc9ae","ac81ccc0520a4374b1e3ff9aedb3b587","c538032f11c94817ac130d1664104f30","ceede5a9dd1946449c4a1e62a93ba743","0cbf3250b24a446dab558206dcf549ca","321ddf20a62246258ca09cb8f34af1d5","c475747244114f418f85f9bcda1ff723","17325faa0c9f43e9987c27156cdc9332","2bbcdf49ad5f427389dab933ff722893","b76fdd8ed39e4d7da58f719fb048c6d7","b2bde6fd09fa426cb1ef43c86ed40870","26f07712b0284f2190f975f0426b951f","afba46655a1644879ef5826e0392ae86","aab35736c09044108ad59108708f7f96","62b9c94ba1594e58b2ff1a651cabddb5"]},"executionInfo":{"elapsed":10406,"status":"ok","timestamp":1698559417643,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-60},"id":"NGCJhY80GqSb","outputId":"72725151-ce98-4da4-cf2d-0d408ed882c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.fingerprint:Parameter 'function'=<function <listcomp>.<lambda> at 0x7ea3c2413b50> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31892793b9934bf1a41050f81978c5d9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fff5ae40b37d4a83a2764f57533e73c8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53809765cd604b0f9e11ab501cf8ac88","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da8cdd9f78b949ecaed2ff6db2ed4c0f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6de100507b1b4f39871e87a2f08ca2a5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0cbf3250b24a446dab558206dcf549ca","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# filter languages\n","train_data = [(language_i, datasets_train.filter(lambda dataset: dataset['language']==language_i)) for language_i in languages]\n","val_data = [(language_i, datasets_val.filter(lambda dataset: dataset['language']==language_i)) for language_i in languages]"]},{"cell_type":"markdown","metadata":{"id":"6x5mkiQsF5Pr"},"source":["## 2. Sequence Labelers Supervised Models"]},{"cell_type":"markdown","metadata":{"id":"0j0Ej5YdF9aw"},"source":["### 2.1. Encoder-Decoder Model"]},{"cell_type":"markdown","metadata":{"id":"YcDiiknyGSh4"},"source":["#### 2.1.1. Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26736,"status":"ok","timestamp":1698787824726,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"ct25EmGTGwef","outputId":"5fe32a09-23db-43e7-9f2b-38d959e4b47e"},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/ar/ar.wiki.bpe.vs100000.model\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2383518/2383518 [00:00<00:00, 2747586.07B/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/ar/ar.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 38037405/38037405 [00:02<00:00, 13110985.74B/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/bn/bn.wiki.bpe.vs100000.model\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2943332/2943332 [00:01<00:00, 2914699.96B/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/bn/bn.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 38121170/38121170 [00:03<00:00, 11533421.33B/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs100000.model\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1959924/1959924 [00:00<00:00, 2237802.79B/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs100000.d100.w2v.bin.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 37930291/37930291 [00:04<00:00, 9099070.60B/s] \n"]}],"source":["# embeddings\n","from bpemb import BPEmb\n","dim_ = 100                                         # embedding vector size\n","vocabulary_ = 100000                               # size vocabulary\n","\n","bpe_models = {\n","    languages[0]: BPEmb(lang='ar', dim=dim_, vs = vocabulary_),\n","    languages[1]: BPEmb(lang='bn', dim=dim_, vs = vocabulary_),\n","    languages[2]: BPEmb(lang='id', dim=dim_, vs = vocabulary_)\n","}\n","\n","PAD_id = vocabulary_\n","EOS_id = vocabulary_+1\n","SOS_id = vocabulary_+2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1qDh_wZHRlN"},"outputs":[],"source":["def span_answer_document(df_list_annotations = [], df_list_documents = []):\n","  \"\"\"Check whether the answer of a question is fully contained\"\"\"\n","  output = []\n","  for x,y in zip(df_list_annotations, df_list_documents):\n","    if x['answer_text'][0] == '':\n","      output.append(0)\n","    else:\n","      start_position = x['answer_start'][0]\n","      end_position = start_position + len(x['answer_text'][0])\n","      if end_position > len(y):\n","        output.append(0)\n","      else:\n","        output.append(1)\n","  return output\n","\n","def oracle(df_list_annotations = []):\n","  \"\"\"Check whether a question has an answer\"\"\"\n","  return [0 if x['answer_text'][0] == '' else 1 for x in df_list_annotations]\n","\n","\n","answerable_train = oracle(datasets_train['annotations'])\n","datasets_train = datasets_train.add_column(\"label\", answerable_train)\n","\n","answerable_val = oracle(datasets_val['annotations'])\n","datasets_val = datasets_val.add_column(\"label\", answerable_val)\n","\n","answerable_fully_document_train = span_answer_document(datasets_train['annotations'], datasets_train['document_plaintext'])\n","datasets_train = datasets_train.add_column(\"full_answer_document\", answerable_fully_document_train)\n","\n","answerable_fully_document_val = span_answer_document(datasets_val['annotations'], datasets_val['document_plaintext'])\n","datasets_val = datasets_val.add_column(\"full_answer_document\", answerable_fully_document_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1698787829548,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"KBB8mIGqIb--","outputId":"22b515cd-5db8-4323-a46a-ca3fd6b6294d"},"outputs":[{"name":"stdout","output_type":"stream","text":["58059\n"]},{"data":{"text/plain":["58059"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["print(sum(datasets_train['label']));sum(datasets_train['full_answer_document'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698787830047,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"t1jGLs5vHyhj","outputId":"20114380-050a-44bf-b37d-145b9af28867"},"outputs":[{"name":"stdout","output_type":"stream","text":["6665\n"]},{"data":{"text/plain":["6665"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["print(sum(datasets_val['label']));sum(datasets_val['full_answer_document'])"]},{"cell_type":"markdown","metadata":{"id":"DfcOHHzdGx3x"},"source":["#### 2.1.2. Model"]},{"cell_type":"markdown","metadata":{"id":"LoS7HKejJ_-z"},"source":["##### 2.1.2.1. Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXcwp5s0J_--"},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    \"\"\"\n","    RNN Encoder model.\n","    \"\"\"\n","    def __init__(self,\n","            pretrained_embeddings: torch.tensor,\n","            lstm_dim: int,\n","            dropout_prob: float = 0.1):\n","        \"\"\"\n","        Initializer for EncoderRNN network\n","        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n","        :param lstm_dim: The dimensionality of the LSTM network\n","        :param dropout_prob: Dropout probability\n","        \"\"\"\n","        # First thing is to call the superclass initializer\n","        super(EncoderRNN, self).__init__()\n","\n","        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n","        # The components are an embedding layer, and an LSTM layer.\n","        self.model = nn.ModuleDict({\n","            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=PAD_id),\n","            'lstm': nn.LSTM(pretrained_embeddings.shape[1],\n","                            lstm_dim,\n","                            2,\n","                            batch_first=True,\n","                            dropout=dropout_prob,\n","                            bidirectional=True,\n","                            dtype=torch.float64),\n","                            })\n","        # Initialize the weights of the model\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        all_params = list(self.model['lstm'].named_parameters())\n","        for n, p in all_params:\n","            if 'weight' in n:\n","                nn.init.xavier_normal_(p)\n","            elif 'bias' in n:\n","                nn.init.zeros_(p)\n","\n","    def forward(self, inputs, input_lens):\n","        \"\"\"\n","        Defines how tensors flow through the model\n","        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n","        :param input_lens: (b) The length of each input sequence\n","        :return: (lstm output state, lstm hidden state)\n","        \"\"\"\n","        embeds = self.model['embeddings'](inputs)\n","        lstm_in = nn.utils.rnn.pack_padded_sequence(\n","                    embeds,\n","                    input_lens.cpu(),\n","                    batch_first=True,\n","                    enforce_sorted=False\n","                )\n","        #lstm_in_2 = lstm_in.data.to(torch.float32)  # Convert data to torch.float32\n","\n","        #lstm_out, hidden_states = self.model['lstm'](lstm_in_2)\n","        lstm_out, hidden_states = self.model['lstm'](lstm_in)\n","        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n","        return lstm_out, hidden_states\n","        #return lstm_in"]},{"cell_type":"markdown","metadata":{"id":"Ic1GISduKJin"},"source":["##### 2.1.2.2. Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n356BpGoJ_-_"},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    \"\"\"\n","    RNN Decoder model.\n","    \"\"\"\n","    def __init__(self, pretrained_embeddings: torch.tensor,\n","            lstm_dim: int,\n","            dropout_prob: float = 0.1,\n","            n_classes: int = 2):\n","        \"\"\"\n","        Initializer for DecoderRNN network\n","        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n","        :param lstm_dim: The dimensionality of the LSTM network\n","        :param dropout_prob: Dropout probability\n","        :param n_classes: Number of prediction classes\n","        \"\"\"\n","        # First thing is to call the superclass initializer\n","        super(DecoderRNN, self).__init__()\n","        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n","        # The components are an embedding layer, a LSTM layer, and a feed-forward output layer\n","        self.model = nn.ModuleDict({\n","            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=PAD_id),\n","            'lstm': nn.LSTM(pretrained_embeddings.shape[1],\n","                            lstm_dim,\n","                            2,\n","                            bidirectional=True,\n","                            dropout=dropout_prob,\n","                            batch_first=True,\n","                            dtype=torch.float64),\n","            'nn': nn.Linear(lstm_dim*2, n_classes, dtype=torch.float64),\n","        })\n","        # Initialize the weights of the model\n","        self._init_weights()\n","        self.dropout = nn.Dropout(p=dropout_prob)\n","\n","\n","    def _init_weights(self):\n","        all_params = list(self.model['lstm'].named_parameters()) + list(self.model['nn'].named_parameters())\n","        for n, p in all_params:\n","            if 'weight' in n:\n","                nn.init.xavier_normal_(p)\n","            elif 'bias' in n:\n","                nn.init.zeros_(p)\n","\n","\n","    def forward(self, inputs, hidden, input_lens):\n","        \"\"\"\n","        Defines how tensors flow through the model\n","        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n","        :param hidden: (b) The hidden state of the previous step\n","        :param input_lens: (b) The length of each input sequence\n","        :return: (output predictions, lstm hidden states) the hidden states will be used as input at the next step\n","        \"\"\"\n","        embeds = self.model['embeddings'](inputs)\n","\n","        lstm_in = nn.utils.rnn.pack_padded_sequence(\n","                    embeds,\n","                    input_lens.cpu(),\n","                    batch_first=True,\n","                    enforce_sorted=False\n","                )\n","        #lstm_in_2 = lstm_in.data.to(torch.float32)  # Convert data to torch.float32\n","        lstm_out, hidden_states = self.model['lstm'](lstm_in, hidden)\n","        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n","        # Max pool along the last dimension\n","        features_lstm = self.dropout(torch.max(lstm_out, 0)[0])\n","        output = self.model['nn'](features_lstm)\n","        return output, hidden_states\n","        #return features_lstm\n","        #return lstm_in"]},{"cell_type":"markdown","metadata":{"id":"DBxNdus5KQQR"},"source":["##### 2.1.2.3. Sequence to Sequence Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6tWxhAOJ_-_"},"outputs":[],"source":["# Define the model\n","class Seq2Seq(nn.Module):\n","    \"\"\"\n","    Basic Seq2Seq network\n","    \"\"\"\n","    def __init__(\n","            self,\n","            pretrained_embeddings: torch.tensor,\n","            lstm_dim: int,\n","            dropout_prob: float = 0.1,\n","            n_classes: int = 2\n","    ):\n","        \"\"\"\n","        Initializer for basic Seq2Seq network\n","        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n","        :param lstm_dim: The dimensionality of the LSTM network\n","        :param dropout_prob: Dropout probability\n","        :param n_classes: The number of output classes\n","        \"\"\"\n","\n","        # First thing is to call the superclass initializer\n","        super(Seq2Seq, self).__init__()\n","\n","        # We'll define the network in a ModuleDict, which consists of an encoder and a decoder\n","        self.model = nn.ModuleDict({\n","            'encoder': EncoderRNN(pretrained_embeddings, lstm_dim, dropout_prob),\n","            'decoder': DecoderRNN(pretrained_embeddings, lstm_dim, dropout_prob, n_classes),\n","        })\n","        self.loss = nn.CrossEntropyLoss()\n","\n","\n","    def forward(self, inputs, input_lens, labels=None):\n","        \"\"\"\n","        Defines how tensors flow through the model.\n","        For the Seq2Seq model this includes 1) encoding the whole input text,\n","        and running *target_length* decoding steps to predict the tag of each token.\n","\n","        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n","        :param input_lens: (b) The length of each input sequence\n","        :param labels: (b) The label of each sample\n","        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n","        \"\"\"\n","\n","        # Get embeddings (b x sl x embedding dim)\n","        encoder_output, encoder_hidden = self.model['encoder'](inputs, input_lens)\n","        decoder_hidden = encoder_hidden  # All Context Encoded\n","        #decoder_input = torch.tensor([SOS_id]*inputs.shape[0], device=device)\n","        #decoder_input = labels[:, 0].unsqueeze(-1) # Label associated with first position\n","        decoder_input = inputs[:, 0].unsqueeze(-1)\n","\n","        mask = (decoder_input != PAD_id)  # Create a mask to identify non-padding elements\n","        input_lens_step = mask.sum(1)  # Compute sequence lengths for each time step\n","\n","        target_length = labels.size(1)\n","\n","        loss = None\n","\n","        for di in range(1,target_length):\n","            decoder_output, decoder_hidden = self.model['decoder'](\n","                decoder_input, decoder_hidden, input_lens_step)\n","\n","            if loss == None:\n","                loss = self.loss(decoder_output.squeeze(1), labels[:, di])\n","            else:\n","                loss += self.loss(decoder_output.squeeze(1), labels[:, di])\n","            # Teacher forcing: Feed the target as the next input\n","            decoder_input = labels[:, di].unsqueeze(-1)\n","            mask = (decoder_input != PAD_id)  # Create a mask to identify non-padding elements\n","            input_lens_step = mask.sum(1)  # Compute sequence lengths for each time step\n","\n","        return loss / target_length"]},{"cell_type":"markdown","metadata":{"id":"NiEXQmrah5kP"},"source":["#### 2.1.3. Model Set Up"]},{"cell_type":"markdown","metadata":{"id":"GSet_7YziKTq"},"source":["##### 2.1.3.1. Functions Preprocessing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhCL1Ggqh81U"},"outputs":[],"source":["def offset_mapping_manual(tokens):\n","  \"\"\"Get the token boundaries (start, end)\"\"\"\n","  token_boundaries = []\n","  start = 0\n","  for token in tokens:\n","      end = start + len(token)\n","      token_boundaries.append((start, end-1))\n","      start = end\n","  return token_boundaries\n","\n","def end_answer(list_annotations=[]):\n","  \"\"\" Check where the answer ends\"\"\"\n","  end_list = []\n","  for x in list_annotations:\n","    start = x['answer_start'][0]\n","    lenght_ = len(x['answer_text'][0])\n","\n","    if start>0:\n","      end = [start + lenght_]\n","    else:\n","      end = [start]\n","    end_list.append(end)\n","  return end_list\n","\n","def list_dummies_start_function(list_annotations = [], list_offset = []):\n","  \"\"\"Look for the token position (touple) where the answer starts\"\"\"\n","  list_dummies_start = []\n","  for x,y in zip(list_annotations, list_offset):\n","    start = x['answer_start'][0]\n","    list_verification = []\n","    for element_first, element_second in y:\n","      if element_first<=start and element_second>=start:\n","        list_verification.append(1)\n","      else:\n","        list_verification.append(0)\n","    list_dummies_start.append(list_verification)\n","  return list_dummies_start\n","\n","def list_dummies_end_function(list_end_answer = [], list_offset = []):\n","  \"\"\"Look for the token position (touple) where the answer ends\"\"\"\n","  list_dummies_end = []\n","  for x,y in zip(list_end_answer, list_offset):\n","    end = x[0]\n","    list_verification = []\n","\n","    for element_first, element_second in y:\n","      if element_first<=end and element_second>=end:\n","        list_verification.append(1)\n","      else:\n","        list_verification.append(0)\n","\n","    list_dummies_end.append(list_verification)\n","  return list_dummies_end\n","\n","def sequence_dummies_documments(list_dummies_start=[], list_dummies_end=[]):\n","  \"\"\" Get dummies for all tokens between the start and the end tokens\"\"\"\n","  condition = 1\n","  list_output = []\n","  for x, y in zip(list_dummies_start, list_dummies_end):\n","    indices_start = [i for i, x in enumerate(x) if x == condition]\n","    indices_end = [i for i, x in enumerate(y) if x == condition]\n","    output = np.repeat(0,len(x))\n","    if len(indices_start)>0:\n","      for index, value in enumerate(output):\n","        try:\n","          if index>=indices_start[0] and index<=indices_end[0]:\n","            output[index] = 1\n","        except:\n","          output[index] = 0\n","    list_output.append(list(output))\n","  return list_output"]},{"cell_type":"markdown","metadata":{"id":"LNq2eDaziOQ-"},"source":["##### 2.1.3.2. Pytorch input format and data loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0COC1CGiUtg"},"outputs":[],"source":["# Define input format for each row in the neural network\n","\n","def text_to_batch_bilstm(text_question: List,\n","                         text_document: List,\n","                         tokenizer, max_len=512,\n","                         id_token_eos = EOS_id,\n","                         id_token_sos = SOS_id) -> Tuple[List, List]:\n","    \"\"\"\n","    Creates a tokenized batch for input to a bilstm model\n","    :param text: A list of sentences to tokenize\n","    :param tokenizer: A tokenization function to use (i.e. fasttext)\n","    :return: Tokenized text as well as the length of the input sequence\n","    \"\"\"\n","    # Some light preprocessing\n","    #input_ids = [id_token_eos]+[tokenizer.encode_ids(t) for t in text_question]+[id_token_eos]+[tokenizer.encode_ids(t) for t in text_document]\n","    input_ids = []\n","    for x, y in zip(text_question, text_document):\n","      input_ids.append([id_token_sos]+tokenizer.encode_ids(x)+[id_token_eos]+tokenizer.encode_ids(y))\n","    return input_ids, [len(ids) for ids in input_ids]\n","\n","\n","# This will load the dataset and process it\n","class ClassificationDatasetReader(Dataset):\n","  def __init__(self, df, tokenizer, column_text_questions, column_text_documents, column_label):\n","    self.df = df\n","    self.tokenizer = tokenizer\n","    self.column_text_questions = column_text_questions\n","    self.column_text_documents = column_text_documents\n","    self.column_label = column_label\n","\n","\n","  def __len__(self):\n","    return len(self.df)\n","\n","  def __getitem__(self, idx):\n","    row = self.df[idx]\n","    # Calls the text_to_batch function\n","    input_ids_column_text_together, seq_lens_column_text_together = text_to_batch_bilstm(text_question = [row[self.column_text_questions]],\n","                                                                                         text_document = [row[self.column_text_documents]],\n","                                                                                         tokenizer = self.tokenizer)\n","\n","    label = row[self.column_label]\n","\n","    return input_ids_column_text_together, seq_lens_column_text_together, label\n","\n","\n","# Prepare data for pytorch object\n","# Asumes the output from text_to_batch_bilstm\n","def collate_batch_bilstm(input_data: Tuple,\n","                         id_pad = PAD_id):\n","\n","    \"\"\"\n","    Combines multiple data samples into a single batch\n","    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n","    :return: A tuple of tensors (input_ids, seq_lens, labels)\n","    \"\"\"\n","    input_ids_ = [i[0][0] for i in input_data]\n","    seq_lens_ = [i[1][0] for i in input_data]\n","    labels = [i[2] for i in input_data]\n","\n","    # Pad all of the input samples to the max length question\n","    max_length = max([len(i) for i in input_ids_])\n","    input_ids_ = [(i + [id_pad] * (max_length - len(i))) for i in input_ids_]\n","    labels = [(i + [0] * (max_length - len(i))) for i in labels]                  # 0 ANSWER IN PADDINGS\n","\n","\n","    # Make sure each sample is max_length long\n","    assert (all(len(i) == max_length for i in input_ids_))\n","    assert (all(len(i) == max_length for i in labels))\n","\n","    return torch.tensor(input_ids_, dtype=torch.int64), torch.tensor(seq_lens_, dtype=torch.int64), torch.tensor(labels)"]},{"cell_type":"markdown","metadata":{"id":"xc1t4Xlsj535"},"source":["##### 2.1.3.3. Functions Train Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V168pms1GWrt"},"outputs":[],"source":["PATH = \"/content/drive/MyDrive/model_lstm_seq\"\n","def train(\n","    model: nn.Module,\n","    train_dl: DataLoader,\n","    valid_dl: DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    n_epochs: int,\n","    device: torch.device,\n","    PATH = PATH\n","):\n","    \"\"\"\n","    The main training loop which will optimize a given model on a given dataset\n","    :param model: The model being optimized\n","    :param train_dl: The training dataset\n","    :param valid_dl: A validation dataset\n","    :param optimizer: The optimizer used to update the model parameters\n","    :param n_epochs: Number of epochs to train for\n","    :param device: The device to train on\n","    :return: (model, losses) The best model and the losses per iteration\n","    \"\"\"\n","\n","    # Keep track of the loss and best accuracy\n","    losses = []\n","    best_f1 = 0.0\n","\n","    # Iterate through epochs\n","    for ep in range(n_epochs):\n","\n","        loss_epoch = []\n","\n","        #Iterate through each batch in the dataloader\n","        for batch in tqdm(train_dl):\n","            # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n","            # things like dropout and layer normalization\n","            model.train()\n","\n","            # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n","            # keeps track of these dynamically in its computation graph so you need to explicitly\n","            # zero them out\n","            optimizer.zero_grad()\n","\n","            # Place each tensor on the GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            input_ids = batch[0]\n","            labels = batch[2]\n","            input_lens = batch[1]\n","\n","            # Pass the inputs through the model, get the current loss and logits\n","            loss = model(input_ids, labels=labels, input_lens=input_lens)\n","            losses.append(loss.item())\n","            loss_epoch.append(loss.item())\n","\n","            # Calculate all of the gradients and weight updates for the model\n","            loss.backward()\n","\n","            # Optional: clip gradients\n","            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Finally, update the weights of the model\n","            optimizer.step()\n","\n","        # Perform inline evaluation at the end of the epoch\n","        #f1 = evaluate(model, valid_dl)\n","        print(f'Train loss: {sum(loss_epoch) / len(loss_epoch)}')\n","        torch.save(model, PATH)\n","\n","        # Keep track of the best model based on the accuracy\n","        #if f1 > best_f1:\n","        #    torch.save(model.state_dict(), 'best_model')\n","        #    best_f1 = f1\n","\n","    return model, losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zA6ZPwc6sSSa"},"outputs":[],"source":["softmax = nn.Softmax(dim=-1)\n","\n","def decode(model, inputs, input_lens, labels=None, beam_size=2):\n","    \"\"\"\n","    Decoding/predicting the labels for an input text by running beam search.\n","\n","    :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n","    :param input_lens: (b) The length of each input sequence\n","    :param labels: (b) The label of each sample\n","    :param beam_size: the size of the beam\n","    :return: predicted sequence of labels\n","    \"\"\"\n","\n","    assert inputs.shape[0] == 1\n","    # first, encode the input text\n","    encoder_output, encoder_hidden = model.model['encoder'](inputs, input_lens)\n","    decoder_hidden = encoder_hidden\n","\n","    # the decoder starts generating after the Begining of Sentence (SOS_id) token\n","    decoder_input = torch.tensor([tokenizer.encode([SOS_id,]),], device=device)\n","    target_length = labels.shape[1]\n","\n","    # we will use heapq to keep top best sequences so far sorted in heap_queue\n","    # these will be sorted by the first item in the tuple\n","    heap_queue = []\n","    heap_queue.append((torch.tensor(0), tokenizer.encode([SOS_id]), decoder_input, decoder_hidden))\n","\n","    # Beam Decoding\n","    for _ in range(target_length):\n","        # print(\"next len\")\n","        new_items = []\n","        # for each item on the beam\n","        for j in range(len(heap_queue)):\n","            # 1. remove from heap\n","            score, tokens, decoder_input, decoder_hidden = heapq.heappop(heap_queue)\n","            # 2. decode one more step\n","            decoder_output, decoder_hidden = model.model['decoder'](\n","                decoder_input, decoder_hidden, torch.tensor([1]))\n","            decoder_output = softmax(decoder_output)\n","            # 3. get top-k predictions\n","            best_idx = torch.argsort(decoder_output[0], descending=True)[0]\n","            # print(decoder_output)\n","            # print(best_idx)\n","            for i in range(beam_size):\n","                decoder_input = torch.tensor([[best_idx[i]]], device=device)\n","\n","                new_items.append((score + decoder_output[0,0, best_idx[i]],\n","                                  tokens + [best_idx[i].item()],\n","                                  decoder_input,\n","                                  decoder_hidden))\n","        # add new sequences to the heap\n","        for item in new_items:\n","          # print(item)\n","            heapq.heappush(heap_queue, item)\n","        # remove sequences with lowest score (items are sorted in descending order)\n","        while len(heap_queue) > beam_size:\n","            heapq.heappop(heap_queue)\n","\n","    final_sequence = heapq.nlargest(1, heap_queue)[0]\n","    assert labels.shape[1] == len(final_sequence[1][1:])\n","    return final_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9PaRtiSsXre"},"outputs":[],"source":["def evaluate(model: nn.Module, valid_dl: DataLoader, beam_size:int = 1):\n","    \"\"\"\n","    Evaluates the model on the given dataset\n","    :param model: The model under evaluation\n","    :param valid_dl: A `DataLoader` reading validation data\n","    :return: The accuracy of the model on the dataset\n","    \"\"\"\n","    # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n","    # layer normalization and dropout\n","    model.eval()\n","    labels_all = []\n","    logits_all = []\n","    tags_all = []\n","\n","    # ALSO IMPORTANT: Don't accumulate gradients during this process\n","    with torch.no_grad():\n","        for batch in tqdm(valid_dl, desc='Evaluation'):\n","            batch = tuple(t.to(device) for t in batch)\n","            input_ids = batch[0]\n","            input_lens = batch[1]\n","            labels = batch[2]\n","\n","            best_seq = decode(model, input_ids, input_lens, labels=labels, beam_size=beam_size)\n","            mask = (input_ids != 0)\n","            labels_all.extend([l for seq,samp in zip(list(labels.detach().cpu().numpy()), input_ids) for l,i in zip(seq,samp) if i != 0])\n","            tags_all += best_seq[1][1:]\n","            # print(best_seq[1][1:], labels)\n","    P, R, F1, _ = precision_recall_fscore_support(labels_all, tags_all, average='macro')\n","    print(confusion_matrix(labels_all, tags_all))\n","    return F1"]},{"cell_type":"markdown","metadata":{"id":"g4B15yEfHxpp"},"source":["#### 2.1.4. Arabic"]},{"cell_type":"markdown","metadata":{"id":"jZpeJiBd-78G"},"source":["##### 2.1.4.1. Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["630efe85b88340da8019991936e1b044","60c490263ed743b796082452c7b285af","12e3a7a9a39542509e0742efc4242668","0d045cabdd98471a87ff5e79e1d71e44","39a4a46ad3414031b712bf490667593e","e522fa3e45ec42aa9be11cf86209c147","eed3d617fa4b43858e7f2fa645e22243","8586d02549354dcd801dafee53d990ec","04bd98cae15c4812b8e1fb5c1f5d31ce","f067d49ee2244cea970a89b90f89ac71","38a7d29f35a94ce99b9eabb5584929a5","3092df151e85404bbdc8dca2b6d1da4f","c5f9c65574264856a87c22aa816e2cfc","38eda30294374c89abe2f04a9b54cbc4","afd9ae41f8ee474dae610213816ec6ef","32daa079d8bd427cad5c216221dea2a2","cf630330577a44a09624aca291958c46","4837dab577864530b30a25b69330f1e9","83e124e0d07749548907aaba8077c8cc","6c4a083274eb43b386b9a5e83ca22a04","78f3b55a15414c01ae77d647a0bf286e","dfded47221dc49e2abad91c03e308201"]},"executionInfo":{"elapsed":3110,"status":"ok","timestamp":1698787838841,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"XMBTcFrfOsoO","outputId":"7c871ea2-bfb7-4f57-9c13-20dfce28c4b9"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.fingerprint:Parameter 'function'=<function <lambda> at 0x7e27b4ab6e60> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"630efe85b88340da8019991936e1b044","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3092df151e85404bbdc8dca2b6d1da4f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["language: arabic\n"]}],"source":["#parameters\n","language_ = languages[0]                          # filter language\n","#lstm_dim = 100                                    # dim neural lstm network\n","\n","# 0. Choose language\n","datasets_train_filter = datasets_train.filter(lambda dataset: dataset[\"language\"]==language_)\n","datasets_val_filter = datasets_val.filter(lambda dataset: dataset[\"language\"]==language_)\n","\n","print('language:', language_);\n","\n","# 1. pretrain embeddings for each language\n","tokenizer = bpe_models[language_]\n","\n","# 3. add index for padding [PAD], END-OF-SENTENCE [EOS], START-OF-SENTENCE [SOS]\n","new_tokens=['[PAD]', '[SOS]' ,'[EOS]']\n","pretrained_embeddings = np.concatenate([bpe_models[language_].emb.vectors,\n","                                        np.zeros(shape=(len(new_tokens),dim_))], axis=0)\n","# 4. Extract the vocab and add extra tokeNS\n","vocabulary = bpe_models[language_].emb.index_to_key + new_tokens"]},{"cell_type":"markdown","metadata":{"id":"N7ME7BUCOIIW"},"source":["##### 2.1.4.2. Answer's tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_m4wYyApx13"},"outputs":[],"source":["# tokens plaindocument to identify tokens of answer\n","tokens_plaintext_train = [tokenizer.encode(x) for x in datasets_train_filter['document_plaintext']]\n","datasets_train_filter = datasets_train_filter.add_column(\"document_plaintext_tokens\", tokens_plaintext_train)\n","\n","tokens_plaintext_val = [tokenizer.encode(x) for x in datasets_val_filter['document_plaintext']]\n","datasets_val_filter = datasets_val_filter.add_column(\"document_plaintext_tokens\", tokens_plaintext_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p9EMCKg9jtJ"},"outputs":[],"source":["offset_mapping_manual_train = [offset_mapping_manual(x) for x in datasets_train_filter['document_plaintext_tokens']]\n","datasets_train_filter = datasets_train_filter.add_column(\"offset_mapping\", offset_mapping_manual_train)\n","\n","offset_mapping_manual_val = [offset_mapping_manual(x) for x in datasets_val_filter['document_plaintext_tokens']]\n","datasets_val_filter = datasets_val_filter.add_column(\"offset_mapping\", offset_mapping_manual_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pykovcMxOwwF"},"outputs":[],"source":["end_position_answer_train = end_answer(datasets_train_filter['annotations'])\n","datasets_train_filter = datasets_train_filter.add_column(\"end_answer\", end_position_answer_train)\n","\n","end_position_answer_val = end_answer(datasets_val_filter['annotations'])\n","datasets_val_filter = datasets_val_filter.add_column(\"end_answer\", end_position_answer_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjUY3qBaW_wt"},"outputs":[],"source":["list_train_start_dummie = list_dummies_start_function(datasets_train_filter['annotations'], datasets_train_filter['offset_mapping'])\n","list_train_end_dummie = list_dummies_end_function(datasets_train_filter['end_answer'], datasets_train_filter['offset_mapping'])\n","list_train_sequence_dummies_documments = sequence_dummies_documments(list_train_start_dummie, list_train_end_dummie)\n","datasets_train_filter = datasets_train_filter.add_column(\"sequence_dummies\", list_train_sequence_dummies_documments)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4wID5cBbDOu"},"outputs":[],"source":["list_val_start_dummie = list_dummies_start_function(datasets_val_filter['annotations'], datasets_val_filter['offset_mapping'])\n","list_val_end_dummie = list_dummies_end_function(datasets_val_filter['end_answer'], datasets_val_filter['offset_mapping'])\n","list_val_sequence_dummies_documments = sequence_dummies_documments(list_val_start_dummie, list_val_end_dummie)\n","datasets_val_filter = datasets_val_filter.add_column(\"sequence_dummies\", list_val_sequence_dummies_documments)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDeoGZcQXwUj"},"outputs":[],"source":["print(list_train_start_dummie[10][:20])\n","print(list_train_end_dummie[10][:20])\n","print(list_train_sequence_dummies_documments[10][:20])\n"]},{"cell_type":"markdown","metadata":{"id":"QxCa61UuP-ac"},"source":["##### 2.1.4.3. Tokenize Questions Together With Documents and Output Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-Y7K2-bfY_W"},"outputs":[],"source":["tokens_questiontext_train = [tokenizer.encode(x) for x in datasets_train_filter['question_text']]\n","datasets_train_filter = datasets_train_filter.add_column(\"question_text_tokens\", tokens_questiontext_train)\n","\n","tokens_questiontext_val = [tokenizer.encode(x) for x in datasets_val_filter['question_text']]\n","datasets_val_filter = datasets_val_filter.add_column(\"question_text_tokens\", tokens_questiontext_val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4LpBVkGgUZd"},"outputs":[],"source":["# Train\n","tokens_together = []\n","for x, y in zip(datasets_train_filter['question_text_tokens'], datasets_train_filter['document_plaintext_tokens']):\n","  tokens_together.append(['[SOS]']+x+['[EOS]']+y)\n","\n","dummies_together = []\n","for x, y in zip(datasets_train_filter['question_text_tokens'], datasets_train_filter['sequence_dummies']):\n","  # we know that the answer is not in the question, so we assign 0 to special tokens and tokens from the question\n","  dummies_together.append([0]+list(np.repeat(0,len(x)))+[0]+y)\n","\n","datasets_train_filter = datasets_train_filter.add_column(\"tokens_together\", tokens_together)\n","datasets_train_filter = datasets_train_filter.add_column(\"dummies_together\", dummies_together)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsxThel4gwWu"},"outputs":[],"source":["# Val\n","tokens_together = []\n","for x, y in zip(datasets_val_filter['question_text_tokens'], datasets_val_filter['document_plaintext_tokens']):\n","  tokens_together.append(['[SOS]']+x+['[EOS]']+y)\n","\n","dummies_together = []\n","for x, y in zip(datasets_val_filter['question_text_tokens'], datasets_val_filter['sequence_dummies']):\n","  dummies_together.append([0]+list(np.repeat(0,len(x)))+[0]+y)\n","\n","datasets_val_filter = datasets_val_filter.add_column(\"tokens_together\", tokens_together)\n","datasets_val_filter = datasets_val_filter.add_column(\"dummies_together\", dummies_together)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmZ5oOPNnYCU"},"outputs":[],"source":["print(datasets_train_filter['dummies_together'][0][0:30])"]},{"cell_type":"markdown","metadata":{"id":"APJINxMTSAkA"},"source":["##### 2.1.4.4. Pytorch input format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozLQwzJ_0JWl"},"outputs":[],"source":["datasets_train_filter.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2pQPSj70boI"},"outputs":[],"source":["index_row = range(0,datasets_train_filter.shape[0])\n","datasets_train_filter = datasets_train_filter.add_column(\"index\", index_row)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aodoTYAgy-2n"},"outputs":[],"source":["# sample arabic, was too long and golab failed many times\n","datasets_train_filter = datasets_train_filter.filter(lambda dataset: dataset[\"index\"]<11000)\n","datasets_train_filter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKKM9rQZFDJJ"},"outputs":[],"source":["batch_size = 1\n","\n","# Create the dataset readers\n","train_dataset = ClassificationDatasetReader(datasets_train_filter,\n","                                            tokenizer=tokenizer,\n","                                            column_text_questions='question_text',\n","                                            column_text_documents='document_plaintext',\n","                                            column_label='dummies_together')\n","# dataset loaded lazily with N workers in parallel\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","# Create the dataset readers\n","val_dataset = ClassificationDatasetReader(datasets_val_filter,\n","                                            tokenizer=tokenizer,\n","                                            column_text_questions='question_text',\n","                                            column_text_documents='document_plaintext',\n","                                            column_label='dummies_together')\n","# dataset loaded lazily with N workers in parallel\n","valid_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)"]},{"cell_type":"markdown","metadata":{"id":"3e-78-7rEuMN"},"source":["##### 2.1.4.5. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_e0Yi1PC61R"},"outputs":[],"source":["# Define some hyperparameters\n","lstm_dim = 100\n","dropout_prob = 0.1\n","batch_size = 1\n","lr = 1e-3\n","n_epochs = 1\n","n_workers = 8\n","# Define Model\n","\n","model = Seq2Seq(\n","    pretrained_embeddings=torch.from_numpy(pretrained_embeddings),\n","    lstm_dim=lstm_dim,\n","    n_classes=2,\n","    dropout_prob=dropout_prob\n",").to(device)\n","\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","valid_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","\n","# Create the optimizer\n","optimizer = Adam(model.parameters(), lr=lr)\n","\n","# Train\n","model_train, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-rLGOVMxxS5"},"outputs":[],"source":["model_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLGRhUMcFiw-"},"outputs":[],"source":["PATH = \"Week 39/(BILSM) ENCODER-DECODER MODEL/model_encoder_decoder_seq_1_ARABIC\"\n","torch.save(model_train,PATH)"]},{"cell_type":"markdown","metadata":{"id":"v57_DXHKZRoj"},"source":["##### 2.1.4.6. Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVI4tZziZXp7"},"outputs":[],"source":["PATH = 'Week 39/(BILSM) ENCODER-DECODER MODEL/model_encoder_decoder_seq_1_ARABIC'\n","# Need to define the class again\n","model_train = torch.load(PATH)\n","softmax = nn.Softmax(dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bsjq_j2GQLXL"},"outputs":[],"source":["#predictions\n","predictions_all = []\n","#real labels\n","labels_all = []\n","\n","beam_size=2\n","model = model_train\n","\n","with torch.no_grad():\n","\n","    for batch in tqdm(valid_dl):\n","        #batch = (t.to(device) for t in batch)\n","        input_ids = torch.tensor(batch[0]).to(\"cuda\")\n","        input_lens = torch.tensor(batch[1]).to(\"cuda\")\n","        labels = torch.tensor(batch[2]).to(\"cuda\")\n","\n","        encoder_output, encoder_hidden = model.model['encoder'](input_ids, input_lens)\n","        decoder_hidden = encoder_hidden\n","\n","        # the decoder starts generating after the Begining of Sentence (SOS_id) token\n","        decoder_input = torch.tensor([SOS_id], device=device).unsqueeze(-1)\n","        target_length = labels.shape[1]\n","\n","        # we will use heapq to keep top best sequences so far sorted in heap_queue\n","        # these will be sorted by the first item in the tuple\n","        heap_queue = []\n","        heap_queue.append((torch.tensor(0),[SOS_id], decoder_input, decoder_hidden))\n","        #heap_queue.append((torch.tensor(0), torch.tensor([SOS_id]), decoder_input, decoder_hidden))\n","\n","\n","        # Beam Decoding\n","        for _ in range(target_length-1):\n","            #print(\"next len\")\n","            new_items = []\n","            # for each item on the beam\n","            for j in range(len(heap_queue)):\n","                # 1. remove from heap\n","                score, tokens, decoder_input, decoder_hidden = heapq.heappop(heap_queue)\n","                # 2. decode one more step\n","                decoder_output, decoder_hidden = model.model['decoder'](\n","                    decoder_input, decoder_hidden, torch.tensor([1]))\n","                decoder_output_soft = softmax(decoder_output)\n","                # 3. get top-k predictions\n","                best_idx = torch.argsort(decoder_output_soft[0], descending=True)\n","                # print(decoder_output)\n","                # print(best_idx)\n","                for i in range(beam_size):\n","                    decoder_input = torch.tensor([[best_idx[i]]], device=device)\n","\n","                    new_items.append((score + decoder_output[0, best_idx[i]],\n","                                      tokens + [best_idx[i].item()],\n","                                      decoder_input,\n","                                      decoder_hidden))\n","            # add new sequences to the heap\n","            for item in new_items:\n","              # print(item)\n","                heapq.heappush(heap_queue, item)\n","            # remove sequences with lowest score (items are sorted in descending order) NO\n","            while len(heap_queue) > beam_size:\n","                heapq.heappop(heap_queue)\n","\n","        final_sequence = heapq.nlargest(1, heap_queue)[0]\n","        predicted_sequence = [0]+final_sequence[1][1:]\n","        # Predictions\n","        predictions_all += predicted_sequence\n","\n","        # Real\n","        labels_all += labels.tolist()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1698128004370,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"z7dEO-LEJ9Q1","outputId":"3a3e9f15-ed74-4000-a20b-7f82afbd5d1d"},"outputs":[{"data":{"text/plain":["array([[26389,   467],\n","       [  337,     8]])"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(labels_all, predictions_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1698128251595,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"hUEXqIitVFVz","outputId":"5bc8a0ed-2834-4a0d-8712-a8d3e310f49f"},"outputs":[{"data":{"text/plain":["0.9873166427704864"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["1-(337+8)/((337+8)+(26389+467))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1698128004372,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"eJy2vMW6LUIt","outputId":"77a6ceef-844e-4d32-9644-f29c272f3004"},"outputs":[{"data":{"text/plain":["0.97044226315209"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["accuracy_score(labels_all, predictions_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1698128004372,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"Fj3RpAMnMDvQ","outputId":"16a5b1f1-2c2a-40c4-f467-9c762581119e"},"outputs":[{"data":{"text/plain":["0.01951219512195122"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["f1_score(labels_all, predictions_all)"]},{"cell_type":"markdown","metadata":{"id":"VGWDZdX7mwl3"},"source":["#### 2.1.5. Bengali"]},{"cell_type":"markdown","metadata":{"id":"HsONQ7ejmwmY"},"source":["##### 2.1.5.1. Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["0f1d1197a9c74a37933ec9d8300f50bc","9ca3729be8064bcca4c878ea81168a4f","2f2428f74d274a1fb4547d7fd6212d3f","9bde047ee13e4fe9942c47355771ff84","8f2b44cf3c0d43849833abf9a27fef5d","ae8596b7e4b34f54876e393dce15d876","97139e555b4040358bae623739f5dfe9","be6575530baa45a6a763ee19f6f24e24","7d9a144022fa44ac80296327fbd52cd3","78f2e53af6c147ee8be5ea557f590c27","17dce79fcff84c3789d97b7a945879c8","05c54fcea24e4034aab781e6b3b39169","1fe0ac76f97a4f679abd972d35683873","1ac6a25806554927b77d6fffe8ea7d54","e1894cf92108468f83e138d5ecf64400","bbfa9febde794f098f447b0018a5a87e","5b99daa73b4843ff922b0a509f407b72","a3b526a7a638420b8199c45a92ec1922","403025eac97b44f389bc4400ca6bd341","e5e5bcd5f18d48a89f5b3bfbdabdd4ea","1ffbbb34680b49ca8b5f5b4f96e3c7d8","39db5e8c4e644f83874d1b9802079c6f"]},"executionInfo":{"elapsed":3426,"status":"ok","timestamp":1698787852667,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"lr4PAxp4mwmY","outputId":"237b23f5-de68-4cda-82e6-e99b773e8175"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f1d1197a9c74a37933ec9d8300f50bc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05c54fcea24e4034aab781e6b3b39169","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["language: bengali\n"]}],"source":["#parameters\n","language_ = languages[1]                          # filter language\n","#lstm_dim = 100                                    # dim neural lstm network\n","\n","# 0. Choose language\n","datasets_train_filter = datasets_train.filter(lambda dataset: dataset[\"language\"]==language_)\n","datasets_val_filter = datasets_val.filter(lambda dataset: dataset[\"language\"]==language_)\n","\n","print('language:', language_);\n","\n","# 1. pretrain embeddings for each language\n","tokenizer = bpe_models[language_]\n","\n","# 3. add index for padding [PAD], END-OF-SENTENCE [EOS], START-OF-SENTENCE [SOS]\n","new_tokens=['[PAD]', '[SOS]' ,'[EOS]']\n","pretrained_embeddings = np.concatenate([bpe_models[language_].emb.vectors,\n","                                        np.zeros(shape=(len(new_tokens),dim_))], axis=0)\n","# 4. Extract the vocab and add extra tokeNS\n","vocabulary = bpe_models[language_].emb.index_to_key + new_tokens"]},{"cell_type":"markdown","metadata":{"id":"Ef_x6CdMmwma"},"source":["##### 2.1.5.2. Answer's tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["f548ad2755cb44b082fe5b73dbd22d53","b0509d277ecb44c0bf0d14b3cf38397a","a964da1208f8419f87295f283ae1e105","4fd5e212be3d4235a63b108310a3a061","8d6bb5ea380e4734acab29994315fd56","63ed01bdc4e84303a3caa9031df442a7","c69b3546a09f439cb22fe0c2c41ebd8e","e072f62986814584b76c1fc3cdca4a93","6243a2d61dd0489a829245806dc40c15","17b9232df19145ac84e2b48df651b3e1","f6806e57b2bc44a9acf4f151e1c2b78d","36d49492b00a444b992659f292913091","3842f4c3e20b4a019b024a77fbe76e09","d007b39fa5c5484f8982c6b3b3de4745","b4d05d53bea145338e17b5576ee823cc","5d3db5a5095c4af1a4ff3bb3b938fcb8","fac6eeb96f834091ac9cb403d7926071","e0b9dd08e71d499891d710e3c08e67b0","c5f4a82586104e988dbf59179b6ca941","cd7740cbe2bd446283ff95cd26877916","461cc3b85deb4dfeafb2a163c2296081","2907c39265904f30b9aaef8c5ee9f17a"]},"executionInfo":{"elapsed":5842,"status":"ok","timestamp":1698787859978,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"OMtnqULtmwmb","outputId":"64701787-e3f5-4400-ff1b-81bdb2eaa872"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f548ad2755cb44b082fe5b73dbd22d53","version_major":2,"version_minor":0},"text/plain":["Flattening the indices:   0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36d49492b00a444b992659f292913091","version_major":2,"version_minor":0},"text/plain":["Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# tokens plaindocument to identify tokens of answer\n","tokens_plaintext_train = [tokenizer.encode(x) for x in datasets_train_filter['document_plaintext']]\n","datasets_train_filter = datasets_train_filter.add_column(\"document_plaintext_tokens\", tokens_plaintext_train)\n","\n","tokens_plaintext_val = [tokenizer.encode(x) for x in datasets_val_filter['document_plaintext']]\n","datasets_val_filter = datasets_val_filter.add_column(\"document_plaintext_tokens\", tokens_plaintext_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTBkx03Dmwmb"},"outputs":[],"source":["offset_mapping_manual_train = [offset_mapping_manual(x) for x in datasets_train_filter['document_plaintext_tokens']]\n","datasets_train_filter = datasets_train_filter.add_column(\"offset_mapping\", offset_mapping_manual_train)\n","\n","offset_mapping_manual_val = [offset_mapping_manual(x) for x in datasets_val_filter['document_plaintext_tokens']]\n","datasets_val_filter = datasets_val_filter.add_column(\"offset_mapping\", offset_mapping_manual_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvIYKJQRmwmc"},"outputs":[],"source":["end_position_answer_train = end_answer(datasets_train_filter['annotations'])\n","datasets_train_filter = datasets_train_filter.add_column(\"end_answer\", end_position_answer_train)\n","\n","end_position_answer_val = end_answer(datasets_val_filter['annotations'])\n","datasets_val_filter = datasets_val_filter.add_column(\"end_answer\", end_position_answer_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87D0cS1Omwme"},"outputs":[],"source":["list_train_start_dummie = list_dummies_start_function(datasets_train_filter['annotations'], datasets_train_filter['offset_mapping'])\n","list_train_end_dummie = list_dummies_end_function(datasets_train_filter['end_answer'], datasets_train_filter['offset_mapping'])\n","list_train_sequence_dummies_documments = sequence_dummies_documments(list_train_start_dummie, list_train_end_dummie)\n","datasets_train_filter = datasets_train_filter.add_column(\"sequence_dummies\", list_train_sequence_dummies_documments)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxhGslL2mwmf"},"outputs":[],"source":["list_val_start_dummie = list_dummies_start_function(datasets_val_filter['annotations'], datasets_val_filter['offset_mapping'])\n","list_val_end_dummie = list_dummies_end_function(datasets_val_filter['end_answer'], datasets_val_filter['offset_mapping'])\n","list_val_sequence_dummies_documments = sequence_dummies_documments(list_val_start_dummie, list_val_end_dummie)\n","datasets_val_filter = datasets_val_filter.add_column(\"sequence_dummies\", list_val_sequence_dummies_documments)"]},{"cell_type":"markdown","metadata":{"id":"DuijSbhHmwmi"},"source":["##### 2.1.5.3. Tokenize Questions Together With Documents and Output Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxI62prRmwmj"},"outputs":[],"source":["tokens_questiontext_train = [tokenizer.encode(x) for x in datasets_train_filter['question_text']]\n","datasets_train_filter = datasets_train_filter.add_column(\"question_text_tokens\", tokens_questiontext_train)\n","\n","tokens_questiontext_val = [tokenizer.encode(x) for x in datasets_val_filter['question_text']]\n","datasets_val_filter = datasets_val_filter.add_column(\"question_text_tokens\", tokens_questiontext_val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_mUVEiCmwmk"},"outputs":[],"source":["# Train\n","tokens_together = []\n","for x, y in zip(datasets_train_filter['question_text_tokens'], datasets_train_filter['document_plaintext_tokens']):\n","  tokens_together.append(['[SOS]']+x+['[EOS]']+y)\n","\n","dummies_together = []\n","for x, y in zip(datasets_train_filter['question_text_tokens'], datasets_train_filter['sequence_dummies']):\n","  # we know that the answer is not in the question, so we assign 0 to special tokens and tokens from the question\n","  dummies_together.append([0]+list(np.repeat(0,len(x)))+[0]+y)\n","\n","datasets_train_filter = datasets_train_filter.add_column(\"tokens_together\", tokens_together)\n","datasets_train_filter = datasets_train_filter.add_column(\"dummies_together\", dummies_together)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYA9C1F2mwml"},"outputs":[],"source":["# Val\n","tokens_together = []\n","for x, y in zip(datasets_val_filter['question_text_tokens'], datasets_val_filter['document_plaintext_tokens']):\n","  tokens_together.append(['[SOS]']+x+['[EOS]']+y)\n","\n","dummies_together = []\n","for x, y in zip(datasets_val_filter['question_text_tokens'], datasets_val_filter['sequence_dummies']):\n","  dummies_together.append([0]+list(np.repeat(0,len(x)))+[0]+y)\n","\n","datasets_val_filter = datasets_val_filter.add_column(\"tokens_together\", tokens_together)\n","datasets_val_filter = datasets_val_filter.add_column(\"dummies_together\", dummies_together)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1698787898764,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"FiFC5f-Cmwml","outputId":"dc2409be-249d-4e1b-fb52-2ae86eacd27d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["print(datasets_train_filter['dummies_together'][0][0:30])"]},{"cell_type":"markdown","metadata":{"id":"aLHO9NNgmwmm"},"source":["##### 2.1.5.4. Pytorch input format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiUMDhBtmwmp"},"outputs":[],"source":["batch_size = 1\n","\n","# Create the dataset readers\n","train_dataset = ClassificationDatasetReader(datasets_train_filter,\n","                                            tokenizer=tokenizer,\n","                                            column_text_questions='question_text',\n","                                            column_text_documents='document_plaintext',\n","                                            column_label='dummies_together')\n","# dataset loaded lazily with N workers in parallel\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","# Create the dataset readers\n","val_dataset = ClassificationDatasetReader(datasets_val_filter,\n","                                            tokenizer=tokenizer,\n","                                            column_text_questions='question_text',\n","                                            column_text_documents='document_plaintext',\n","                                            column_label='dummies_together')\n","# dataset loaded lazily with N workers in parallel\n","valid_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)"]},{"cell_type":"markdown","metadata":{"id":"oeaZ5XUBmwmq"},"source":["##### 2.1.5.5. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSUjq7UkxBcp"},"outputs":[],"source":["# Define some hyperparameters\n","lstm_dim = 100\n","dropout_prob = 0.1\n","lr = 1e-3\n","n_epochs = 10\n","n_workers = 8\n","# Define Model\n","\n","model = Seq2Seq(\n","    pretrained_embeddings=torch.from_numpy(pretrained_embeddings),\n","    lstm_dim=lstm_dim,\n","    n_classes=2,\n","    dropout_prob=dropout_prob\n",").to(device)\n","\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","valid_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","\n","# Create the optimizer\n","optimizer = Adam(model.parameters(), lr=lr)\n","\n","# Train\n","model_train, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698056271534,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"G2wFQyJxmwmr","outputId":"24b0d28c-b763-428e-a802-609dc06bf22b"},"outputs":[{"data":{"text/plain":["Seq2Seq(\n","  (model): ModuleDict(\n","    (encoder): EncoderRNN(\n","      (model): ModuleDict(\n","        (embeddings): Embedding(100003, 100, padding_idx=100000)\n","        (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n","      )\n","    )\n","    (decoder): DecoderRNN(\n","      (model): ModuleDict(\n","        (embeddings): Embedding(100003, 100, padding_idx=100000)\n","        (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n","        (nn): Linear(in_features=200, out_features=2, bias=True)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (loss): CrossEntropyLoss()\n",")"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["model_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2668,"status":"ok","timestamp":1698056280034,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"dIsinxEtmwms","outputId":"f641e669-fe68-4c3a-d89c-5bfba7513592"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["PATH = \"Week 39/(BILSM) ENCODER-DECODER MODEL/model_encoder_decoder_seq_1_BENGALI\"\n","torch.save(model_train,PATH)"]},{"cell_type":"markdown","metadata":{"id":"5TEVGH_Zmwmt"},"source":["##### 2.1.5.6. Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6LkU5QGnzNi"},"outputs":[],"source":["PATH = 'Week 39/(BILSM) ENCODER-DECODER MODEL/model_encoder_decoder_seq_1_BENGALI'\n","# Need to define the class again\n","model_train = torch.load(PATH)\n","softmax = nn.Softmax(dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6Os9Vckp96s"},"outputs":[],"source":["#predictions\n","predictions_all = []\n","#real labels\n","labels_all = []\n","\n","beam_size=2\n","model = model_train\n","\n","with torch.no_grad():\n","\n","    for batch in tqdm(valid_dl):\n","        #batch = (t.to(device) for t in batch)\n","        input_ids = torch.tensor(batch[0]).to(\"cuda\")\n","        input_lens = torch.tensor(batch[1]).to(\"cuda\")\n","        labels = torch.tensor(batch[2]).to(\"cuda\")\n","\n","        encoder_output, encoder_hidden = model.model['encoder'](input_ids, input_lens)\n","        decoder_hidden = encoder_hidden\n","\n","        # the decoder starts generating after the Begining of Sentence (SOS_id) token\n","        decoder_input = torch.tensor([SOS_id], device=device).unsqueeze(-1)\n","        target_length = labels.shape[1]\n","\n","        # we will use heapq to keep top best sequences so far sorted in heap_queue\n","        # these will be sorted by the first item in the tuple\n","        heap_queue = []\n","        heap_queue.append((torch.tensor(0),[SOS_id], decoder_input, decoder_hidden))\n","        #heap_queue.append((torch.tensor(0), torch.tensor([SOS_id]), decoder_input, decoder_hidden))\n","\n","\n","        # Beam Decoding\n","        for _ in range(target_length-1):\n","            #print(\"next len\")\n","            new_items = []\n","            # for each item on the beam\n","            for j in range(len(heap_queue)):\n","                # 1. remove from heap\n","                score, tokens, decoder_input, decoder_hidden = heapq.heappop(heap_queue)\n","                # 2. decode one more step\n","                decoder_output, decoder_hidden = model.model['decoder'](\n","                    decoder_input, decoder_hidden, torch.tensor([1]))\n","                decoder_output_soft = softmax(decoder_output)\n","                # 3. get top-k predictions\n","                best_idx = torch.argsort(decoder_output_soft[0], descending=True)\n","                # print(decoder_output)\n","                # print(best_idx)\n","                for i in range(beam_size):\n","                    decoder_input = torch.tensor([[best_idx[i]]], device=device)\n","\n","                    new_items.append((score + decoder_output[0, best_idx[i]],\n","                                      tokens + [best_idx[i].item()],\n","                                      decoder_input,\n","                                      decoder_hidden))\n","            # add new sequences to the heap\n","            for item in new_items:\n","              # print(item)\n","                heapq.heappush(heap_queue, item)\n","            # remove sequences with lowest score (items are sorted in descending order) NO\n","            while len(heap_queue) > beam_size:\n","                heapq.heappop(heap_queue)\n","\n","        final_sequence = heapq.nlargest(1, heap_queue)[0]\n","        predicted_sequence = [0]+final_sequence[1][1:]\n","        # Predictions\n","        predictions_all += predicted_sequence\n","\n","        # Real\n","        labels_all += labels.tolist()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83,"status":"ok","timestamp":1698126079264,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"UahgIyzqqcRG","outputId":"a708b5e8-2f19-494c-f740-67bd8a4df782"},"outputs":[{"data":{"text/plain":["array([[26843,    13],\n","       [  345,     0]])"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(labels_all, predictions_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77,"status":"ok","timestamp":1698126079264,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"D1ntRrfrqcRj","outputId":"2f387003-d2be-497d-ab0f-c99106a5f751"},"outputs":[{"data":{"text/plain":["0.9868387191647366"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["accuracy_score(labels_all, predictions_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1698126079612,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"yKSClumJqcRk","outputId":"48ee9d8c-e179-4a96-d3db-d003b6c0622f"},"outputs":[{"data":{"text/plain":["0.0"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["f1_score(labels_all, predictions_all)"]},{"cell_type":"markdown","metadata":{"id":"TuasxVAcrErH"},"source":["#### 2.1.6. Indonesian"]},{"cell_type":"markdown","metadata":{"id":"KW4p9BYNrEro"},"source":["##### 2.1.6.1. Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["84d25daeda16413b92c0443382659c03","8f571e64c70d418e8f3556581ec178a4","e5ff0d5b84a24f46b501734d4248e9f0","2cdb3d1d94b048289505a36df714258b","4a3d9e98f2144c6992a35cf991015678","fcc7dfa675fa48f38dbdb8d7af748d64","039c629e89b84b518d66515073e201c7","c11e83aa55a14fe5bc18d3505d9cb1c5","dfd32dbf334f4685bdc4a55df475b3ce","2aa675d9e6e54247aa3a8b0aab6d1038","b53f9ed7c49f44b6826ab1949f6dfb12","34edcafe22e34eb2af01c2e6cc9d867c","89d1a01557684b4c979e1c0be2f4d66e","06b0706c5ae84e9b9fab11c78f76feef","85bb74f964e24cfda15d1b62add91055","e500e986c1c8417dbcd941332f938f08","1f458fabd374489890ab070e501a5032","afce79b98ba4415cbd0044bc21d88874","aa17017b49874f6292ff44c1f8ef0a17","640826849f6047519936174824c61d1a","99e185f909db4849ab8e5eb489e585c3","f754e7ad93b44a62b036718f9b97d0d8"]},"executionInfo":{"elapsed":5379,"status":"ok","timestamp":1698789883375,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"ObfSsSwMrEro","outputId":"0a6fec0d-184c-411e-ab9a-204dc5585143"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84d25daeda16413b92c0443382659c03","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34edcafe22e34eb2af01c2e6cc9d867c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["language: indonesian\n"]}],"source":["#parameters\n","language_ = languages[2]                          # filter language\n","#lstm_dim = 100                                    # dim neural lstm network\n","\n","# 0. Choose language\n","datasets_train_filter = datasets_train.filter(lambda dataset: dataset[\"language\"]==language_)\n","datasets_val_filter = datasets_val.filter(lambda dataset: dataset[\"language\"]==language_)\n","\n","print('language:', language_);\n","\n","# 1. pretrain embeddings for each language\n","tokenizer = bpe_models[language_]\n","\n","# 3. add index for padding [PAD], END-OF-SENTENCE [EOS], START-OF-SENTENCE [SOS]\n","new_tokens=['[PAD]', '[SOS]' ,'[EOS]']\n","pretrained_embeddings = np.concatenate([bpe_models[language_].emb.vectors,\n","                                        np.zeros(shape=(len(new_tokens),dim_))], axis=0)\n","# 4. Extract the vocab and add extra tokeNS\n","vocabulary = bpe_models[language_].emb.index_to_key + new_tokens"]},{"cell_type":"markdown","metadata":{"id":"CuSeQrOgrErq"},"source":["##### 2.1.6.2. Answer's tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["0f75cf95541a439081f6d3d4a863468c","58f00d9f2c8a442cbeafb03404686ac1","71f7aa41391b4eb9a50183c3c3b36566","00b9291d5bed45be873f6a9f2980f26c","ce9b00b313624f648606d0c33ea4a7b6","37e8d462f3c6443bb2eb34338beb194d","37e1e1ef06634dd884c8253d480f42d7","038e6f9ff0b94184a240c3892dc36f03","4e61f1dd27ce4fad99d866159e51b733","f679e4ef454944d5ab1ffd8c75dcdaa1","cb69a52bd42e47ca923e66d1299cb2c6","b95a42a3b208403ea2c17e1cb2d853e6","42bf5d36a1e44fc1a6d238f6fb54ab54","06570ef118e14ff39b3b483341c9e381","43b8c5fdfeaf487b935aa8418697934e","cbb7e4afe90e48108d74d5749ca0d7d3","80e5452149f6470b9dbeaa0fcdaf9c6e","0974d9a7591f4edbbb1fd7c829c1b74e","e0565ad0f0394d25915451e3fbece13d","292a178d2faf4dff8e512713ffc204a8","4e66218e3f3244c8a85b4f115d128651","9a876f8b221841a2bd0ce4232e315311"]},"executionInfo":{"elapsed":9017,"status":"ok","timestamp":1698789892387,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"YPyuHuEcrErq","outputId":"3733bbcf-e5e8-421c-8c1a-abc70f559ba1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f75cf95541a439081f6d3d4a863468c","version_major":2,"version_minor":0},"text/plain":["Flattening the indices:   0%|          | 0/12 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b95a42a3b208403ea2c17e1cb2d853e6","version_major":2,"version_minor":0},"text/plain":["Flattening the indices:   0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# tokens plaindocument to identify tokens of answer\n","tokens_plaintext_train = [tokenizer.encode(x) for x in datasets_train_filter['document_plaintext']]\n","datasets_train_filter = datasets_train_filter.add_column(\"document_plaintext_tokens\", tokens_plaintext_train)\n","\n","tokens_plaintext_val = [tokenizer.encode(x) for x in datasets_val_filter['document_plaintext']]\n","datasets_val_filter = datasets_val_filter.add_column(\"document_plaintext_tokens\", tokens_plaintext_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXFlsaRUrErr"},"outputs":[],"source":["offset_mapping_manual_train = [offset_mapping_manual(x) for x in datasets_train_filter['document_plaintext_tokens']]\n","datasets_train_filter = datasets_train_filter.add_column(\"offset_mapping\", offset_mapping_manual_train)\n","\n","offset_mapping_manual_val = [offset_mapping_manual(x) for x in datasets_val_filter['document_plaintext_tokens']]\n","datasets_val_filter = datasets_val_filter.add_column(\"offset_mapping\", offset_mapping_manual_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18nJzjKtrErs"},"outputs":[],"source":["end_position_answer_train = end_answer(datasets_train_filter['annotations'])\n","datasets_train_filter = datasets_train_filter.add_column(\"end_answer\", end_position_answer_train)\n","\n","end_position_answer_val = end_answer(datasets_val_filter['annotations'])\n","datasets_val_filter = datasets_val_filter.add_column(\"end_answer\", end_position_answer_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gF0Vkh_rErs"},"outputs":[],"source":["list_train_start_dummie = list_dummies_start_function(datasets_train_filter['annotations'], datasets_train_filter['offset_mapping'])\n","list_train_end_dummie = list_dummies_end_function(datasets_train_filter['end_answer'], datasets_train_filter['offset_mapping'])\n","list_train_sequence_dummies_documments = sequence_dummies_documments(list_train_start_dummie, list_train_end_dummie)\n","datasets_train_filter = datasets_train_filter.add_column(\"sequence_dummies\", list_train_sequence_dummies_documments)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4cC23dxrErt"},"outputs":[],"source":["list_val_start_dummie = list_dummies_start_function(datasets_val_filter['annotations'], datasets_val_filter['offset_mapping'])\n","list_val_end_dummie = list_dummies_end_function(datasets_val_filter['end_answer'], datasets_val_filter['offset_mapping'])\n","list_val_sequence_dummies_documments = sequence_dummies_documments(list_val_start_dummie, list_val_end_dummie)\n","datasets_val_filter = datasets_val_filter.add_column(\"sequence_dummies\", list_val_sequence_dummies_documments)"]},{"cell_type":"markdown","metadata":{"id":"tjy_0ovQrEru"},"source":["##### 2.1.6.3. Tokenize Questions Together With Documents and Output Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGAQp0ZUrEru"},"outputs":[],"source":["tokens_questiontext_train = [tokenizer.encode(x) for x in datasets_train_filter['question_text']]\n","datasets_train_filter = datasets_train_filter.add_column(\"question_text_tokens\", tokens_questiontext_train)\n","\n","tokens_questiontext_val = [tokenizer.encode(x) for x in datasets_val_filter['question_text']]\n","datasets_val_filter = datasets_val_filter.add_column(\"question_text_tokens\", tokens_questiontext_val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9vxvTtJrErv"},"outputs":[],"source":["# Train\n","tokens_together = []\n","for x, y in zip(datasets_train_filter['question_text_tokens'], datasets_train_filter['document_plaintext_tokens']):\n","  tokens_together.append(['[SOS]']+x+['[EOS]']+y)\n","\n","dummies_together = []\n","for x, y in zip(datasets_train_filter['question_text_tokens'], datasets_train_filter['sequence_dummies']):\n","  # we know that the answer is not in the question, so we assign 0 to special tokens and tokens from the question\n","  dummies_together.append([0]+list(np.repeat(0,len(x)))+[0]+y)\n","\n","datasets_train_filter = datasets_train_filter.add_column(\"tokens_together\", tokens_together)\n","datasets_train_filter = datasets_train_filter.add_column(\"dummies_together\", dummies_together)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMDsByqIrErw"},"outputs":[],"source":["# Val\n","tokens_together = []\n","for x, y in zip(datasets_val_filter['question_text_tokens'], datasets_val_filter['document_plaintext_tokens']):\n","  tokens_together.append(['[SOS]']+x+['[EOS]']+y)\n","\n","dummies_together = []\n","for x, y in zip(datasets_val_filter['question_text_tokens'], datasets_val_filter['sequence_dummies']):\n","  dummies_together.append([0]+list(np.repeat(0,len(x)))+[0]+y)\n","\n","datasets_val_filter = datasets_val_filter.add_column(\"tokens_together\", tokens_together)\n","datasets_val_filter = datasets_val_filter.add_column(\"dummies_together\", dummies_together)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":738,"status":"ok","timestamp":1698789975007,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"s6HZmcGQrErw","outputId":"f7e94ca0-a5df-429d-999e-cc7e71939a11"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["print(datasets_train_filter['dummies_together'][0][0:30])"]},{"cell_type":"markdown","metadata":{"id":"hrNaUbeKrErx"},"source":["##### 2.1.6.4. Pytorch input format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNGwuM6DrErx"},"outputs":[],"source":["batch_size = 1\n","\n","# Create the dataset readers\n","train_dataset = ClassificationDatasetReader(datasets_train_filter,\n","                                            tokenizer=tokenizer,\n","                                            column_text_questions='question_text',\n","                                            column_text_documents='document_plaintext',\n","                                            column_label='dummies_together')\n","# dataset loaded lazily with N workers in parallel\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","# Create the dataset readers\n","val_dataset = ClassificationDatasetReader(datasets_val_filter,\n","                                            tokenizer=tokenizer,\n","                                            column_text_questions='question_text',\n","                                            column_text_documents='document_plaintext',\n","                                            column_label='dummies_together')\n","# dataset loaded lazily with N workers in parallel\n","valid_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)"]},{"cell_type":"markdown","metadata":{"id":"ngwzI3wCrEry"},"source":["##### 2.1.6.5. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165,"referenced_widgets":["e07f0b2c09774463952801cc95eca025","0f08d8dd50554cc796a62ab684855ad2","cc318bd561ff4314b8d2ecf9b2d9e554","33e8b8b6535d47298a83e53f15c58c5d","4a3af54923464a0aa66165796abbde43","7507d0413d26485d81efb8894e1e526e","2262e5b0799d4222bcc8033c927415c6","ba171defdfc04d86837975bdf22f48b7","fd56b2f5d170404e9c6eebd3ad8451dc","92c6bd10e7da4d3a8a6cdfbb6a84ef8f","f48caf225f6d489b843e1917dd79e5e6","69bf3655df784f50a82214f141e53212","cf68396244dc4ee7bb1a7a2ae4a17aa7","c483892eb1f241709f7c9f6c21d178f3","98eb8c44b6f2483e851d46fa395b890b","fbdd7c577bdf4c249b6f698e329b87f2","b41c9d4ce9514832ac435bc81f9014e0","4cc957331b154d7f90289858807164ac","f60b6c7dad1f47a7aeadd5f3570d4a34","97331615b21a4bcbacdff4adcf9679c2","ccc345562fc84e40908490702669a767","47ab2c62aad64ba599f2ec3d03575acf","8a7487210bd04cbab4b37929610a6e11","ee0d280f479f46389f519e8b5000e993","9a07ce470fb0407880972a23e5ac3f91","71b96528d1f24547acf88c57dffad6a7","15f706edc91d40b6a490bf075d7bd89e","9940d18364c94e8588539734ef7781e9","32125ed7ebec40ea823538bbfef95d47","4ce51131c17d4cc79181b62b14a6cc9d","23642251e6d148bdb3d173449a270044","2084aab56f0f4424988071492a1134ea","fb85410d9e2e499aad9c3c543a6bec8f"]},"executionInfo":{"elapsed":8864655,"status":"ok","timestamp":1697710765901,"user":{"displayName":"GROUP 14","userId":"12105126576464516058"},"user_tz":-120},"id":"ArT8vgmmsHqP","outputId":"86ab0e52-f9fe-4854-e0d8-2a012e1dc65c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e07f0b2c09774463952801cc95eca025","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/11394 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Train loss: 0.049261860013665545\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69bf3655df784f50a82214f141e53212","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/11394 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Train loss: 0.04524049638616121\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a7487210bd04cbab4b37929610a6e11","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/11394 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Train loss: 0.04348221039761484\n"]}],"source":["# Define some hyperparameters\n","lstm_dim = 100\n","dropout_prob = 0.1\n","lr = 1e-3\n","n_epochs = 3\n","n_workers = 8\n","# Define Model\n","\n","model = Seq2Seq(\n","    pretrained_embeddings=torch.from_numpy(pretrained_embeddings),\n","    lstm_dim=lstm_dim,\n","    n_classes=2,\n","    dropout_prob=dropout_prob\n",").to(device)\n","\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","valid_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n","                      collate_fn=collate_batch_bilstm, num_workers=8)\n","\n","\n","# Create the optimizer\n","optimizer = Adam(model.parameters(), lr=lr)\n","\n","# Train\n","model_train, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698056271534,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"BEWozNTirEr0","outputId":"24b0d28c-b763-428e-a802-609dc06bf22b"},"outputs":[{"data":{"text/plain":["Seq2Seq(\n","  (model): ModuleDict(\n","    (encoder): EncoderRNN(\n","      (model): ModuleDict(\n","        (embeddings): Embedding(100003, 100, padding_idx=100000)\n","        (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n","      )\n","    )\n","    (decoder): DecoderRNN(\n","      (model): ModuleDict(\n","        (embeddings): Embedding(100003, 100, padding_idx=100000)\n","        (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n","        (nn): Linear(in_features=200, out_features=2, bias=True)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (loss): CrossEntropyLoss()\n",")"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["model_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2668,"status":"ok","timestamp":1698056280034,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"fFalFXAgrEr0","outputId":"f641e669-fe68-4c3a-d89c-5bfba7513592"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["PATH = \"Week 39/(BILSM) ENCODER-DECODER MODEL/model_encoder_decoder_seq_1_INDONESIAN\"\n","torch.save(model_train,PATH)"]},{"cell_type":"markdown","metadata":{"id":"G99H4etZrEr2"},"source":["##### 2.1.5.6. Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8viInKd0rEr2"},"outputs":[],"source":["PATH = 'Week 39/(BILSM) ENCODER-DECODER MODEL/model_encoder_decoder_seq_1_INDONESIAN'\n","# Need to define the class again\n","model_train = torch.load(PATH)\n","softmax = nn.Softmax(dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["15135de7cc36433c85af179adc9a56e7","64ee4c6f73e34d97aa56d245fb7f7899","c9dfc00401154bbb85803d22377008bc","356d0104f03b4f0dbfcf9808f486c465","8c90eb71612d466eac2f9059b9109b1b","3fc944cda1c04f54a165182576978d46","bd2053ba16524715b48ff27694abc9f6","ade28c7f982e48b5bc67e831c6e53595","0db16ee410bf484582d59ef2a6c93b5b","fb462ae5264e491c81f5abe9990e91d7","7313a2cc67d341569dd184c4a174bc49"]},"executionInfo":{"elapsed":382407,"status":"ok","timestamp":1698790362316,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"chWgjn1mrEr3","outputId":"cd656d22-aa81-4671-8c00-48d986e0acbc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15135de7cc36433c85af179adc9a56e7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1191 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#predictions\n","predictions_all = []\n","#real labels\n","labels_all = []\n","\n","beam_size=2\n","model = model_train\n","\n","with torch.no_grad():\n","\n","    for batch in tqdm(valid_dl):\n","        #batch = (t.to(device) for t in batch)\n","        input_ids = torch.tensor(batch[0]).to(\"cuda\")\n","        input_lens = torch.tensor(batch[1]).to(\"cuda\")\n","        labels = torch.tensor(batch[2]).to(\"cuda\")\n","\n","        encoder_output, encoder_hidden = model.model['encoder'](input_ids, input_lens)\n","        decoder_hidden = encoder_hidden\n","\n","        # the decoder starts generating after the Begining of Sentence (SOS_id) token\n","        decoder_input = torch.tensor([SOS_id], device=device).unsqueeze(-1)\n","        target_length = labels.shape[1]\n","\n","        # we will use heapq to keep top best sequences so far sorted in heap_queue\n","        # these will be sorted by the first item in the tuple\n","        heap_queue = []\n","        heap_queue.append((torch.tensor(0),[SOS_id], decoder_input, decoder_hidden))\n","        #heap_queue.append((torch.tensor(0), torch.tensor([SOS_id]), decoder_input, decoder_hidden))\n","\n","\n","        # Beam Decoding\n","        for _ in range(target_length-1):\n","            #print(\"next len\")\n","            new_items = []\n","            # for each item on the beam\n","            for j in range(len(heap_queue)):\n","                # 1. remove from heap\n","                score, tokens, decoder_input, decoder_hidden = heapq.heappop(heap_queue)\n","                # 2. decode one more step\n","                decoder_output, decoder_hidden = model.model['decoder'](\n","                    decoder_input, decoder_hidden, torch.tensor([1]))\n","                decoder_output_soft = softmax(decoder_output)\n","                # 3. get top-k predictions\n","                best_idx = torch.argsort(decoder_output_soft[0], descending=True)\n","                # print(decoder_output)\n","                # print(best_idx)\n","                for i in range(beam_size):\n","                    decoder_input = torch.tensor([[best_idx[i]]], device=device)\n","\n","                    new_items.append((score + decoder_output[0, best_idx[i]],\n","                                      tokens + [best_idx[i].item()],\n","                                      decoder_input,\n","                                      decoder_hidden))\n","            # add new sequences to the heap\n","            for item in new_items:\n","              # print(item)\n","                heapq.heappush(heap_queue, item)\n","            # remove sequences with lowest score (items are sorted in descending order) NO\n","            while len(heap_queue) > beam_size:\n","                heapq.heappop(heap_queue)\n","\n","        final_sequence = heapq.nlargest(1, heap_queue)[0]\n","        predicted_sequence = [0]+final_sequence[1][1:]\n","        # Predictions\n","        predictions_all += predicted_sequence\n","\n","        # Real\n","        labels_all += labels.tolist()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1698790362316,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"raOgGft-st4L","outputId":"0a26d8a1-7599-404b-9091-39bf89a806d6"},"outputs":[{"data":{"text/plain":["array([[117337,     25],\n","       [  3422,      0]])"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(labels_all, predictions_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1698790362317,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"n2hp2oicst4M","outputId":"5d273f20-487b-457a-b0d6-a3c197351a7b"},"outputs":[{"data":{"text/plain":["0.9714614518479269"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["accuracy_score(labels_all, predictions_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1698790362738,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"q2SCB9TCst4N","outputId":"26de9b33-d5e8-49c0-f2b0-83fa49ac9908"},"outputs":[{"data":{"text/plain":["0.0"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["f1_score(labels_all, predictions_all)"]},{"cell_type":"markdown","metadata":{"id":"CuVB6UcMGRaU"},"source":["### 2.2. Transformer Multilingual BERT"]},{"cell_type":"markdown","metadata":{"id":"8YT6YiLkGXk0"},"source":["#### 2.2.1. Pre-processing (Functions)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":2634,"status":"ok","timestamp":1698851034807,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"mJDPh8itEtDs"},"outputs":[],"source":["checkpoint = \"bert-base-multilingual-uncased\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","# Define function to tokenize question and documents together and the output\n","def get_train_features(samples):\n","  '''\n","  Tokenizes the text in the given samples, splittling inputs that are too long\n","  for our model across multiple features. Finds the token offsets of the answers,\n","  which ____ the labels for our inputs.\n","  '''\n","  answers = samples[\"annotations\"]\n","  start_positions = []\n","  end_positions = []\n","  y_sequence = []\n","\n","\n","  batch = tokenizer(\n","        samples['question_text'],\n","        samples['document_plaintext'],\n","        truncation=\"only_second\",\n","        stride=128,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","  # Since one document might give several features if it is long\n","  # we need a mapping that shows what example each feature is associated with.\n","  sample_mapping = batch.pop('overflow_to_sample_mapping')\n","\n","  # This gives a map from token to character position in the original context\n","  # helps us computer start and end positions.\n","  offset_mapping = batch.pop('offset_mapping')\n","\n","  id_words_list_special_characters = batch.word_ids()\n","\n","  for i, offset in enumerate(offset_mapping):\n","      sample_idx = sample_mapping[i]                                                # id for identifying the row\n","      answer = answers[sample_idx]                                                  # answer associated with that id\n","      start_char = answer[\"answer_start\"][0]                                        # position character where answer starts\n","      end_char = answer[\"answer_start\"][0] + len(answer[\"answer_text\"][0])          # position character where answer finishes\n","      sequence_ids = batch.sequence_ids(i)                                         # identify question, answer, special characters (EOS, PADDING, etc)\n","\n","      # Find the start and end of the context\n","      idx = 0\n","      while sequence_ids[idx] != 1:                                                 # identify question characters or special characters\n","          idx += 1\n","      context_start = idx                                                           # identify beggining of context\n","      while sequence_ids[idx] == 1:\n","          idx += 1\n","      context_end = idx - 1                                                         # identify end of context\n","\n","      # If the answer is not fully inside the context, label is (0, 0)\n","      if offset[context_start][0] > start_char or offset[context_end][1] < end_char: # when truncating, if the first part of the context is after the answe or if the last part of the context is before the end of the answer\n","          start_positions.append(0)\n","          end_positions.append(0)\n","      else:\n","          # Otherwise it's the start and end token positions\n","          idx = context_start\n","          while idx <= context_end and offset[idx][0] <= start_char:                  # between the start of the answer\n","              idx += 1\n","          start_positions.append(idx - 1)\n","\n","          idx = context_end\n","          while idx >= context_start and offset[idx][1] >= end_char:                  # between the end of the answer\n","              idx -= 1\n","          end_positions.append(idx + 1)\n","\n","      y_sequence_loop = [0] * len(offset)\n","\n","      for index, token in enumerate(offset):\n","        if (start_positions[i]<=index)&(end_positions[i]>=index):\n","          y_sequence_loop[index] = 1\n","        if token == (0,0):\n","          y_sequence_loop[index] = -100\n","\n","      y_sequence.append(y_sequence_loop)\n","\n","  batch['labels']  = y_sequence\n","  return batch"]},{"cell_type":"markdown","metadata":{"id":"pbikfKNmFIxF"},"source":["#### 2.2.2. Model"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2375,"status":"ok","timestamp":1698851038700,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"lyLXaHTtGd4P","outputId":"bcd81384-2c03-4f4e-ece7-2dfa9ed4866f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["label_names = ['no answer', 'answer']\n","\n","id2label = {'0':'no answer', '1': 'answer'}\n","label2id = {v: k for k, v in id2label.items()}\n","\n","model = AutoModelForTokenClassification.from_pretrained(checkpoint,\n","                                                       id2label=id2label,\n","                                                       label2id=label2id,)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1698851038701,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"owej4ogOz8Wg","outputId":"978f780f-e6a9-42da-efe9-68536f9bb5c9"},"outputs":[{"data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"qSdTgT8dGxkc"},"source":["#### 2.2.3. Model Set Up"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":1257,"status":"ok","timestamp":1698851041251,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"Pc4wSwvpGv-Y"},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# hyperparameters\n","path=f\"/content/drive/MyDrive/train\"\n","args = TrainingArguments(\n","    path,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","# Evaluation metric\n","metric = evaluate.load(\"seqeval\")\n","def compute_metrics(eval_preds):\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    # Remove ignored index (special tokens) and convert to labels\n","    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": all_metrics[\"overall_precision\"],\n","        \"recall\": all_metrics[\"overall_recall\"],\n","        \"f1\": all_metrics[\"overall_f1\"],\n","        \"accuracy\": all_metrics[\"overall_accuracy\"],\n","    }"]},{"cell_type":"markdown","metadata":{"id":"OWxzduO_I3Rm"},"source":["#### 2.2.4. Arabic"]},{"cell_type":"markdown","metadata":{"id":"17qS9eXhKRL_"},"source":["##### 2.2.4.1. Pre-process (Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["ac58a6ad68924aacaa07a6fc35822565","2ca45871a80e48908ccef0727b3c7b90","4e40eacff71b45d5811571d711bee155","a20be4b890194b39bede5b44a0a31c06","f4f632c439874c939750088fdb6c8f1c","f28cebe963144c52a7b596dbf595e7f9","790dc6392bd54c43b5c5aca1c64142b5","1ca7850305ae460cb363b8c0c142c654","a7ee6fe4776a421cbb46a0d21550f6b0","b527bda41e2c462688f1100f7fc82de8","beaf3b4e6a20406093b645801b620e85","f4a80247109e445a99e074a41d5f9971","62e2f403882a4900a4a8c926d06fdbaf","5067c356f686436ba5e79db8ce4a2f77","79ade0eb0a72493ca82adb65cb5db285","4ac65ab2b52c47c7b23a1d7c1d25afb8","44d2c9ace0ed404fb15fbd78b823914c","8ce4480f1bbe4667aab119b6a9a1e970","7115ce966e06445fa0fef67d04c5db7f","1cf8657c9b52439ab5de1352cf6fb22a","796975fc733f4976a749f0ff80648a4c","5cc4d78674ad42fe8eafc8e1110b72cb"]},"executionInfo":{"elapsed":3740,"status":"ok","timestamp":1698234650109,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"u0qIYyNRMUBA","outputId":"8270f7be-530a-4b27-c137-ae2b0c112f10"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.fingerprint:Parameter 'function'=<function <lambda> at 0x7963814fef80> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac58a6ad68924aacaa07a6fc35822565","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4a80247109e445a99e074a41d5f9971","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["language: arabic\n"]}],"source":["#parameters\n","language_ = languages[0]                          # filter language\n","\n","# 0. Choose language\n","datasets_train_filter = datasets_train.filter(lambda dataset: dataset[\"language\"]==language_)\n","datasets_val_filter = datasets_val.filter(lambda dataset: dataset[\"language\"]==language_)\n","\n","print('language:', language_);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["c1c6100833b54a4ba4e912e19a65699e","23a609811d4442f3ba62915bd774bbfb","f5547ee6de62490f9d3d8eaf57da7cc6","ce1f79f8d52149908050087fdf20f317","91733879564147238f0132f0b36c904b","c9de6622a70b4311a0119933d2e6bf11","3b7ada78fe724f159f666b9559143601","5fbfbfac1174477fb64be1d4468168b4","8ef3afe6746c409a89ba12efcc1d1fe8","2755ef3b754541caa47ece5b235ac2b7","e5973c6b68d7425382c3af331e7d1fc6","e7405ec8f89d436c849e2e6d0aab8441","70ca5432ef2c490cb3ef2a76e67bfea8","004a75778c644facbedf3d81a88520b7","5ad8af63b912405494ea96c81a3e4b7d","16a03806154941a7a0813b035dfcca81","0c11489940f243cca066313e4b6fdfac","104cb88cf6aa46c3a2c420a1d1811aa8","7766251c13af4a9ab41269072e7d7c19","f6acf496befe49daaede2d78e6611d79","0ba7307ac2364eac8d5157a374bae9dc","da7ff47554374063949437e73384705a"]},"executionInfo":{"elapsed":36726,"status":"ok","timestamp":1698234703722,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"JwUyrH91zt3a","outputId":"055172aa-2d18-45b3-c1f4-e204dfb7949c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1c6100833b54a4ba4e912e19a65699e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/30 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7405ec8f89d436c849e2e6d0aab8441","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = datasets_train_filter.map(get_train_features, batched = True, remove_columns = datasets_train_filter.column_names)\n","val_dataset = datasets_val_filter.map(get_train_features, batched = True, remove_columns = datasets_train_filter.column_names)"]},{"cell_type":"markdown","metadata":{"id":"AwAoaLMnKjqn"},"source":["##### 2.2.4.2. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"executionInfo":{"elapsed":9162942,"status":"ok","timestamp":1698243999366,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"6OMcIW328JKZ","outputId":"f4633eb6-0f65-4bff-a1e5-d5ad4c0b8e25"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 31187\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 11697\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11697' max='11697' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11697/11697 2:32:34, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.051900</td>\n","      <td>0.045399</td>\n","      <td>0.615621</td>\n","      <td>0.672788</td>\n","      <td>0.642937</td>\n","      <td>0.983452</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.039600</td>\n","      <td>0.045852</td>\n","      <td>0.622051</td>\n","      <td>0.688826</td>\n","      <td>0.653738</td>\n","      <td>0.983970</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.027200</td>\n","      <td>0.050292</td>\n","      <td>0.636970</td>\n","      <td>0.691671</td>\n","      <td>0.663194</td>\n","      <td>0.984653</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1963\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/arabic_sequence/checkpoint-3899\n","Configuration saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3899/config.json\n","Model weights saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3899/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3899/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3899/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1963\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/arabic_sequence/checkpoint-7798\n","Configuration saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7798/config.json\n","Model weights saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7798/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7798/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7798/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1963\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/arabic_sequence/checkpoint-11697\n","Configuration saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11697/config.json\n","Model weights saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11697/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11697/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11697/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=11697, training_loss=0.04288217003675488, metrics={'train_runtime': 9157.5224, 'train_samples_per_second': 10.217, 'train_steps_per_second': 1.277, 'total_flos': 2.4447185856976896e+16, 'train_loss': 0.04288217003675488, 'epoch': 3.0})"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3984,"status":"ok","timestamp":1698244003256,"user":{"displayName":"Alan Forero","userId":"16491346535004947075"},"user_tz":-120},"id":"J5PS259u7mBF","outputId":"1f487de1-7714-4c4b-fe5e-df6dfab022d9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/BERT - ARABIC - SEQUENCE\n","Configuration saved in /content/drive/MyDrive/BERT - ARABIC - SEQUENCE/config.json\n","Model weights saved in /content/drive/MyDrive/BERT - ARABIC - SEQUENCE/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/BERT - ARABIC - SEQUENCE/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/BERT - ARABIC - SEQUENCE/special_tokens_map.json\n"]}],"source":["path = \"/content/drive/MyDrive/BERT - ARABIC - SEQUENCE\"\n","trainer.save_model(path)"]},{"cell_type":"markdown","metadata":{"id":"DaWcEL0RLAoO"},"source":["#### 2.2.5. Bengali"]},{"cell_type":"markdown","metadata":{"id":"JI-rGNRVLAos"},"source":["##### 2.2.5.1. Pre-process (Data)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":99,"referenced_widgets":["f5196c975d844e1a8368eb049ae80e72","064ba2960219489fba35c5211509bf53","7e8adfda96804338b735ff68b0af21af","711d184b921245d188a6c38b88340bd6","7626c601c41540bba989bc9b9ffb838f","63a4db1dfe534ac2a5bd880f1f2cd218","aae8e5671e9c41f6a675a6f62336d25c","59994fdbc3554bf7ab3730f1041944f6","497b9727367b482e81de5ea19ae5738e","6cc7a8b0b695471aafddbef08ca98935","be4636e01b6342f3afaa110d036448bc","ac8a0ca4001f4f0392c29116b33e5a6c","642be3bcd99146f597b927f65e9e6306","485c925610864aecb62225ecc60385fb","8a51bd7900ec49a48f907bc17096e484","6fdef91fa51a47fb9375cab9b8ece79a","181603bcf6de49aaa846bf42d397099f","cc5d9d95271c4ffd96632f4521f63206","5faf977e2b634cc0aaaddd44a0505c7c","60802dd06c934ae49916eae72d225e12","85337737701d4c3298b9d40f347f9308","bfe6a33019ca41c6ad163bd8e109ca2c"]},"executionInfo":{"elapsed":4524,"status":"ok","timestamp":1698851048442,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"3J07fIhBLAot","outputId":"97717a47-4f4b-40b2-bff3-1283074f5347"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5196c975d844e1a8368eb049ae80e72","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac8a0ca4001f4f0392c29116b33e5a6c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["language: bengali\n"]}],"source":["#parameters\n","language_ = languages[1]                          # filter language\n","\n","# 0. Choose language\n","datasets_train_filter = datasets_train.filter(lambda dataset: dataset[\"language\"]==language_)\n","datasets_val_filter = datasets_val.filter(lambda dataset: dataset[\"language\"]==language_)\n","\n","print('language:', language_);"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["e8279088aa2e492b80149a0e42263917","ea7727cc19a5486a8b6f6bdef3900f6b","3f3d23d5ebfc4d8d9745d0a89179599f","03c5d5f80b4e4c09bc968bfb3e95b608","50a5d49e45f04d39a899c8fcbb0d0e68","493da70fad564dabae9c2bcc6a35f507","4953ad6e0acd4679a287b120a284bd0c","08d8822084964988ad59d7dca84dfe7d","7c4bd2a0a1de4186b2d067d44bf031d1","1438f2760f224ea98098222cb6bd392a","ba73b11b236b4601851a0e725bf56714","74da72c0186f40c6a296c999382a5ca1","24fe6bcd4f4d42b5bffee1c1855e9412","3cf623e487a342e5aade961e2041a45f","09a4646b0da04b8ca6a9ed667d35fd21","b9ac04fd8b7244ad9d11360004dd0db1","43ff7e3c15df4a29a396f27640263f54","9a5cf6a1ba164d4587af1a20b72e3faa","6b23e2725e58491fbe8f50ef9cf4ac4c","e78a093c51c44a8cb8cc493155273fac","5a55780e4b4242b994ec4c9ba3f0cfd8","46971b04e2984a7b9444156fdf521c78"]},"executionInfo":{"elapsed":6113,"status":"ok","timestamp":1698851056150,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"S4fGaCUILU1r","outputId":"af3199db-840b-43fb-a4ba-79a88153de70"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8279088aa2e492b80149a0e42263917","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74da72c0186f40c6a296c999382a5ca1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = datasets_train_filter.map(get_train_features, batched = True, remove_columns = datasets_train_filter.column_names)\n","val_dataset = datasets_val_filter.map(get_train_features, batched = True, remove_columns = datasets_train_filter.column_names)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1698851057940,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-60},"id":"PLLUh2rtXeZ0","outputId":"da006bb8-f483-4747-94ab-808bf1c50b06"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","    num_rows: 5197\n","})"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"markdown","metadata":{"id":"4X4sw9CALAov"},"source":["##### 2.2.5.2. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"executionInfo":{"elapsed":1632357,"status":"ok","timestamp":1698224782301,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"sQzxJwyNLt5q","outputId":"eb52aa68-b0e5-4783-ada2-f7706b9e8a58"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 5197\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1950\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1950' max='1950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1950/1950 27:07, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.044900</td>\n","      <td>0.031714</td>\n","      <td>0.410460</td>\n","      <td>0.554604</td>\n","      <td>0.471767</td>\n","      <td>0.988442</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.023800</td>\n","      <td>0.024912</td>\n","      <td>0.554890</td>\n","      <td>0.595289</td>\n","      <td>0.574380</td>\n","      <td>0.991705</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.019200</td>\n","      <td>0.027110</td>\n","      <td>0.582834</td>\n","      <td>0.625268</td>\n","      <td>0.603306</td>\n","      <td>0.992392</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 241\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/bengali_sequence/checkpoint-650\n","Configuration saved in /content/drive/MyDrive/bengali_sequence/checkpoint-650/config.json\n","Model weights saved in /content/drive/MyDrive/bengali_sequence/checkpoint-650/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-650/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-650/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 241\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/bengali_sequence/checkpoint-1300\n","Configuration saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1300/config.json\n","Model weights saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1300/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1300/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1300/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 241\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/bengali_sequence/checkpoint-1950\n","Configuration saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1950/config.json\n","Model weights saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1950/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1950/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1950/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=1950, training_loss=0.02545681268740923, metrics={'train_runtime': 1631.0284, 'train_samples_per_second': 9.559, 'train_steps_per_second': 1.196, 'total_flos': 4073877734270976.0, 'train_loss': 0.02545681268740923, 'epoch': 3.0})"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4209,"status":"ok","timestamp":1698224786470,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"1_L55wQnMKnO","outputId":"1bd485f8-911d-4865-9b2f-e2811023842a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/BERT - BENGALI - SEQUENCE\n","Configuration saved in /content/drive/MyDrive/BERT - BENGALI - SEQUENCE/config.json\n","Model weights saved in /content/drive/MyDrive/BERT - BENGALI - SEQUENCE/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/BERT - BENGALI - SEQUENCE/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/BERT - BENGALI - SEQUENCE/special_tokens_map.json\n"]}],"source":["path = \"/content/drive/MyDrive/BERT - BENGALI - SEQUENCE\"\n","trainer.save_model(path)"]},{"cell_type":"markdown","metadata":{"id":"y_hO2B4NMe9k"},"source":["#### 2.2.6. Indonesian"]},{"cell_type":"markdown","metadata":{"id":"dZyHkKT0Me-A"},"source":["##### 2.2.6.1. Pre-process (Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["68260a9824f64439a12085b271c04491","a4ba694e89384b3599dd19e4eedb374e","9f98310f22ab4a85ba97e1cb9d14d4af","4d360d8fbdb7475fb0ee7c71b7503324","f446bd6e0d66494cbc05a13b72ddfe65","458eab230d9a497ebc9ae7335192265c","f5bb6a0369db429bb29f87c78d218947","0c75024213a34241ac3aa7b6ba2d58a2","9848c4ecd1e4412fabdad707f3cd3e6b","305a5b160b114d5dbea0892875f1f92b","a12b03fcedf74bc3a8bc81e44e5149b7","2ddb9e56d73f448f881201fb8e80790a","d1224372e5524b22989a25eb1d069d0a","02481fd3c16d412f87dd5f0738fdbf77","a3da4588d73b4b96b17d04e5ddb1bb97","acb247a5977d4e5a80422a04f692f9c9","f46fa7d4671d4d609f3213da91d596a4","cbe7071b467648c88a7e028caf3a47c0","95fd055672f748c6a40ac8a06442d672","72a11c4b9d9e484aa20a0269907aadb5","032df150f2214bc08163ba8f9f7ab8be","f5a00904313049c4be05cb20f122ff93"]},"executionInfo":{"elapsed":5609,"status":"ok","timestamp":1698225136713,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"sZBPaTJcM5qx","outputId":"442945cc-9f29-45ce-9c7d-3b76a473886d"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.fingerprint:Parameter 'function'=<function <lambda> at 0x7adfd26eb7f0> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68260a9824f64439a12085b271c04491","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/117 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ddb9e56d73f448f881201fb8e80790a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["language: indonesian\n"]}],"source":["#parameters\n","language_ = languages[2]                          # filter language\n","#lstm_dim = 100                                    # dim neural lstm network\n","\n","# 0. Choose language\n","datasets_train_filter = datasets_train.filter(lambda dataset: dataset[\"language\"]==language_)\n","datasets_val_filter = datasets_val.filter(lambda dataset: dataset[\"language\"]==language_)\n","\n","print('language:', language_);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["52db4ed292e94daa80105ee1f2fcab7c","d59cba63c49646679a2aff28338bfd7f","3f28fbc3a9f34ea68daeead7876cd054","0faa2b9d076f4ca5be1c9ebbf2bf78ed","71892d363a894298ac1045a471fab8bc","846077cf1f3b4c43bb516ed7331141de","7e60f3feba4846ed805eb5fa4bf50f1b","b809b4d85f0846909c83706b062c8954","01427b5926d24a29ae75122c60fc28ed","272b12dd211b4dd9ac0b294b2dc77fa8","8970735d19d344f5a5e423e613e3a0a1","40ad4b1f51fa4d1d9260703990dd6d9b","1ede96e7aca8446ca8cb729c5bc5b7f0","39b2b1b26f484d76be73931e5b48073f","da4275a8063d41a8954c6d8932b76e31","c8180d50d1384537a53b0a082d64e669","aa1f9a68a06a44bd9f93cf61a5302dd4","13ab1fd0bef445ff8288cb650e6cd011","e5cb8a51800743ba8d23d1d51979f1ba","d29f5ee8e8194e72ad5cc23c147e0dfb","780b8ec5e90c4a749c66e7a6348927a0","9c61299722854ebfb9f49d42073aa3e5"]},"executionInfo":{"elapsed":12226,"status":"ok","timestamp":1698225158324,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"-yBAN16lM-mc","outputId":"4c12c451-70a8-4eb0-ca40-334faed32b5c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52db4ed292e94daa80105ee1f2fcab7c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40ad4b1f51fa4d1d9260703990dd6d9b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = datasets_train_filter.map(get_train_features, batched = True, remove_columns = datasets_train_filter.column_names)\n","val_dataset = datasets_val_filter.map(get_train_features, batched = True, remove_columns = datasets_train_filter.column_names)"]},{"cell_type":"markdown","metadata":{"id":"tSzJfQubMe-D"},"source":["##### 2.2.6.2. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":880},"executionInfo":{"elapsed":3555451,"status":"ok","timestamp":1698228794990,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"ex3ZYn06NF5F","outputId":"c6c02775-1a1e-4fc9-dc39-9f3d6ff62831"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 11594\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4350\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4350' max='4350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4350/4350 59:10, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.068800</td>\n","      <td>0.047461</td>\n","      <td>0.593028</td>\n","      <td>0.608822</td>\n","      <td>0.600821</td>\n","      <td>0.984175</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.039300</td>\n","      <td>0.043088</td>\n","      <td>0.638673</td>\n","      <td>0.648772</td>\n","      <td>0.643683</td>\n","      <td>0.985111</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.024000</td>\n","      <td>0.047272</td>\n","      <td>0.621560</td>\n","      <td>0.686226</td>\n","      <td>0.652294</td>\n","      <td>0.984777</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1210\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/indonesian_sequence/checkpoint-1450\n","Configuration saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1450/config.json\n","Model weights saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1450/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1450/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1450/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1210\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/indonesian_sequence/checkpoint-2900\n","Configuration saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2900/config.json\n","Model weights saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2900/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2900/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2900/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1210\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/indonesian_sequence/checkpoint-4350\n","Configuration saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4350/config.json\n","Model weights saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4350/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4350/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4350/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=4350, training_loss=0.04820252769294826, metrics={'train_runtime': 3555.1016, 'train_samples_per_second': 9.784, 'train_steps_per_second': 1.224, 'total_flos': 9088423792791552.0, 'train_loss': 0.04820252769294826, 'epoch': 3.0})"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3529,"status":"ok","timestamp":1698228818910,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"},"user_tz":-120},"id":"F-WY83uSNUKe","outputId":"eed92f78-9110-49f4-8e4e-6e1591f65425"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/BERT - INDONESIAN - SEQUENCE\n","Configuration saved in /content/drive/MyDrive/BERT - INDONESIAN - SEQUENCE/config.json\n","Model weights saved in /content/drive/MyDrive/BERT - INDONESIAN - SEQUENCE/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/BERT - INDONESIAN - SEQUENCE/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/BERT - INDONESIAN - SEQUENCE/special_tokens_map.json\n"]}],"source":["path = \"/content/drive/MyDrive/BERT - INDONESIAN - SEQUENCE\"\n","trainer.save_model(path)"]},{"cell_type":"markdown","metadata":{"id":"jD6cMPy-GYHb"},"source":["### 2.3. Transformer RoBERTa"]},{"cell_type":"markdown","metadata":{"id":"Sn7jI0UrPwZ0"},"source":["#### 2.3.1. Pre-procesing (Functions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["748709b835ec4b739add75ab191336d0","77fa8c9024ac48a4b2aa8da7d42530fa","2d5e548aa2e04c59877c3648914ee7e6","31c788b070e34a01b2cc97695a1159d4","3fd544560e5b48f7811b44099ce55f19","38a03a842f6e40e5b7e70f73850ec850","f04664e1b5ad4849bfd36cf778d53dfb","0bfcd013ef4d49309fa3fb299bca0d98","254cfab9e4c741138658d4c0010eaa80","5d5b071fdd5649b7a7512d06d0795068","e08af64fdf974bcb961f79c900280a06","946e6ebfba0f4666b087cf95d14d503f","19b6937ddc584418b3f5876336ceb8d7","3d18f8efec584cee800dc71c928a1fb3","46484606263d4f7881e32a473115c200","fc501afce71c4bc282b876383d3685af","7f9c79958e1f45c79e96245b10b43a07","62e7c76fdf314bd3b1b68bbf19bbe45c","3080450c45fc47df8da447b1d30eec78","01ce09b3565342949aea96873e315693","0153b5d58adc48d7999713875a7dfaf8","8c6e2b9c55544c06bc398e4e8a033b6f","6d2552fb8aa0464c808d1022c2df7685","d978f00fa9ae45a8bf1beeff5d807af7","aa08fa53ec2045a4a673adbd2141a499","175be4a1a5fd40699f27e6a8ef8c61e0","f026e1db8d074349905074cf96ce3e20","0d14ba4f69e54402ae5a12a84ddf5055","436d46841a4c4231909f6fc6f3b01c8c","eee72bf0690648379e8cf884a66f9d4e","b652f74b282940ec883a274f02fe04d6","b00c05c898d94681a0b3cb1f6e951ed2","9ef7721309ab4312b592f2241d72922f"]},"executionInfo":{"elapsed":5853,"status":"ok","timestamp":1698559431642,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-60},"id":"6u1WBifPygF2","outputId":"747c799a-3580-4d2d-89fa-5b8c3324a5ae"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"748709b835ec4b739add75ab191336d0","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"946e6ebfba0f4666b087cf95d14d503f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d2552fb8aa0464c808d1022c2df7685","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["MODEL_NAME = 'xlm-roberta-base'\n","\n","# Load the pre-trained auto tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","# tokenize and create features\n","def get_train_features(samples):\n","\n","  '''\n","  Tokenizes the text in the given samples, splittling inputs that are too long\n","  for our model across multiple features. Finds the token offsets of the answers,\n","  which helps us find the labels for our inputs.\n","  '''\n","\n","  answers = samples[\"annotations\"]\n","  start_positions = []\n","  end_positions = []\n","  y_sequence = []\n","\n","  batch = tokenizer(\n","        samples['question_text'],\n","        samples['document_plaintext'],\n","        truncation=\"only_second\",\n","        stride=128,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","  # Since one document might give several features if it is long\n","  # we need a mapping that shows what example each feature is associated with.\n","  sample_mapping = batch.pop('overflow_to_sample_mapping')\n","\n","  # This gives a map from token to character position in the original context\n","  # helps us computer start and end positions.\n","  offset_mapping = batch.pop('offset_mapping')\n","\n","  for i, offset in enumerate(offset_mapping):\n","      sample_idx = sample_mapping[i]                                                # id for identifying the row\n","      answer = answers[sample_idx]                                                  # answer associated with that id\n","      start_char = answer[\"answer_start\"][0]                                        # position character where answer starts\n","      end_char = answer[\"answer_start\"][0] + len(answer[\"answer_text\"][0])          # position character where answer finishes\n","      sequence_ids = batch.sequence_ids(i)                                          # identify question, answer, special characters (EOS, PADDING, etc)\n","\n","      # Find the start and end of the context\n","      idx = 0\n","      while sequence_ids[idx] != 1:                                                 # identify question characters or special characters\n","          idx += 1\n","      context_start = idx                                                           # identify beggining of context\n","      while sequence_ids[idx] == 1:\n","          idx += 1\n","      context_end = idx - 1                                                         # identify end of context\n","\n","      # If the answer is not fully inside the context, label is (0, 0)\n","      if offset[context_start][0] > start_char or offset[context_end][1] < end_char: # when truncating, if the first part of the context is after the answe or if the last part of the context is before the end of the answer\n","          start_positions.append(0)\n","          end_positions.append(0)\n","      else:\n","          # Otherwise it's the start and end token positions\n","          idx = context_start\n","          while idx <= context_end and offset[idx][0] <= start_char:                  # between the start of the answer\n","              idx += 1\n","          start_positions.append(idx - 1)\n","\n","          idx = context_end\n","          while idx >= context_start and offset[idx][1] >= end_char:                  # between the end of the answer\n","              idx -= 1\n","          end_positions.append(idx + 1)\n","\n","      y_sequence_loop = [0] * len(offset)\n","\n","      for index, token in enumerate(offset):\n","        if (start_positions[i]<=index)&(end_positions[i]>=index):\n","          y_sequence_loop[index] = 1\n","        if token == (0,0):\n","          y_sequence_loop[index] = -100\n","\n","      y_sequence.append(y_sequence_loop)\n","\n","  batch['labels']  = y_sequence\n","  return batch"]},{"cell_type":"markdown","metadata":{"id":"n9V7qyo9v6zn"},"source":["#### 2.3.2. Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["1529f5eaf1324016a0a32d7ce96835c3","310e48d167164bf7a0dc4c8a6e4056e8","ccafc5b66dda44b9ba447aecfc31ef84","50e3d7aa05e34f2b84e5390a1eb10e91","deae48fb013448b0a6584fd351001be4","6df28e94176d4764b40650acc77e0281","9c57af4eaaa140a5b419321605d78fcd","b13ccfe9a9e84961b17321c563dc9d58","d8a81f8201e544ca8152c65fb157286c","a08e670b7c864299bd81b31c8c76f312","0735c20dddfa43e38f1f8b8fb13008af"]},"executionInfo":{"elapsed":26505,"status":"ok","timestamp":1698559488894,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-60},"id":"t_LeiGBWyZzT","outputId":"e9a44bfa-efef-4ad2-b4ab-677c92912d86"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1529f5eaf1324016a0a32d7ce96835c3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["label_names = ['no answer', 'answer']\n","\n","# load optimizer\n","id2label = {'0':'no answer', '1': 'answer'}\n","label2id = {v: k for k, v in id2label.items()}\n","\n","# load model\n","model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME,\n","                                                       id2label=id2label,\n","                                                       label2id=label2id,)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8845,"status":"ok","timestamp":1698559498657,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-60},"id":"ItoxTPR5_V4R","outputId":"a7b0cbe8-cb69-476b-d460-8a804c991f45"},"outputs":[{"data":{"text/plain":["XLMRobertaForTokenClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# send model to GPU\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"1jD-qk__v3lf"},"source":["#### 2.3.3. Model Set Up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Mhd4cGCwBb_"},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# hyperparameters\n","path=f\"/content/drive/MyDrive/train\"\n","args = TrainingArguments(\n","    path,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","# Evaluation metric\n","metric = evaluate.load(\"seqeval\")\n","def compute_metrics(eval_preds):\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    # Remove ignored index (special tokens) and convert to labels\n","    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": all_metrics[\"overall_precision\"],\n","        \"recall\": all_metrics[\"overall_recall\"],\n","        \"f1\": all_metrics[\"overall_f1\"],\n","        \"accuracy\": all_metrics[\"overall_accuracy\"],\n","    }"]},{"cell_type":"markdown","metadata":{"id":"iZRZ0ONnwKMp"},"source":["#### 2.3.4. Arabic"]},{"cell_type":"markdown","metadata":{"id":"0ZcOsiP9wNjy"},"source":["##### 2.3.4.1. Pre-process (Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["96f5efa0a1fb45ddab45dfb19666a420","05e8da6feed94fab965a3962ae6c6fcf","077d945bc3994ad7bd507ccbfef3da11","55031dc921104ad19d66becf460a8173","04ae2cd373d448bab7ac7ccf402cd5dd","1f08e5f463cc418f91b0180d3f94134b","b48b38716e0e4d79b7cfa38debe3063f","7f87794367824f3d810aa6984cd7408e","93cfb1d199e44757af5c2bb4596a42a5","b777de5248b0406683bdccf9f85c8c0c","2734472432ab4084b942426f3af52932","05c14d2699b64ccbacbedfb766df27df","923802be101741ab824ace8e160cace0","111d38fa82ba415b8d1c62bfe00f48f0","e9e5bdd72bbc43c6ad00225df42a3703","a71da69ff864403f9b90b3e103195924","02170dc40ea442b58d4ab0a769724dcd","b162edd5ccb1407c93bef1d948a76083","3a9269c543c7438a9dadc88c8d6315b4","cad2fb19016b493ca9cd7c679d29900f","1a8ba2920a6f45d5bc734f4978a0634f","8fadbeaccf5e4c7e94dab2a05f74a280"]},"executionInfo":{"elapsed":30769,"status":"ok","timestamp":1698559462405,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-60},"id":"dYDJ3sH8oloI","outputId":"e14069ed-2ce2-4f9f-cf9a-fd95e99e5088"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96f5efa0a1fb45ddab45dfb19666a420","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/30 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05c14d2699b64ccbacbedfb766df27df","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# run the tokenizer\n","tokenized_train_arabic = train_data[0][1].map(partial(get_train_features),\n","                                              batched = True,\n","remove_columns = train_data[0][1].column_names)\n","\n","tokenized_val_arabic = val_data[0][1].map(partial(get_train_features),\n","                                              batched = True,\n","remove_columns = val_data[0][1].column_names)"]},{"cell_type":"markdown","metadata":{"id":"_22hZ-8bxBRH"},"source":["##### 2.3.4.2. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":924},"executionInfo":{"elapsed":8541717,"status":"ok","timestamp":1698569467476,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-60},"id":"lb-Tdgvj_8Vc","outputId":"7eb532f7-4803-4441-e937-f79aa1418619"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 30714\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 11520\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1745' max='11520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 1745/11520 23:42 < 2:12:55, 1.23 it/s, Epoch 0.45/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11520' max='11520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11520/11520 2:46:04, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.060000</td>\n","      <td>0.050017</td>\n","      <td>0.625000</td>\n","      <td>0.603381</td>\n","      <td>0.614000</td>\n","      <td>0.982863</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.047700</td>\n","      <td>0.048749</td>\n","      <td>0.632290</td>\n","      <td>0.693628</td>\n","      <td>0.661540</td>\n","      <td>0.982845</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.039300</td>\n","      <td>0.049721</td>\n","      <td>0.663027</td>\n","      <td>0.694928</td>\n","      <td>0.678603</td>\n","      <td>0.983658</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1947\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/arabic_sequence/checkpoint-3840\n","Configuration saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3840/config.json\n","Model weights saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3840/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3840/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-3840/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1947\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/arabic_sequence/checkpoint-7680\n","Configuration saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7680/config.json\n","Model weights saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7680/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7680/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-7680/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1947\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/arabic_sequence/checkpoint-11520\n","Configuration saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11520/config.json\n","Model weights saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11520/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11520/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/arabic_sequence/checkpoint-11520/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=11520, training_loss=0.05382153172459867, metrics={'train_runtime': 9968.2916, 'train_samples_per_second': 9.244, 'train_steps_per_second': 1.156, 'total_flos': 2.407640575916851e+16, 'train_loss': 0.05382153172459867, 'epoch': 3.0})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# load trainer\n","trainer = Trainer(\n","    model=model,\n","    args = args,\n","    train_dataset=tokenized_train_arabic,\n","    eval_dataset=tokenized_val_arabic,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34174,"status":"ok","timestamp":1698569501654,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-60},"id":"hqUZoLGDBDG2","outputId":"ea138eff-1eb3-4d75-d71c-e5f8b9584929"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/RoBERTa - ARABIC - SEQUENCE\n","Configuration saved in /content/drive/MyDrive/RoBERTa - ARABIC - SEQUENCE/config.json\n","Model weights saved in /content/drive/MyDrive/RoBERTa - ARABIC - SEQUENCE/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/RoBERTa - ARABIC - SEQUENCE/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/RoBERTa - ARABIC - SEQUENCE/special_tokens_map.json\n"]}],"source":["path = \"/content/drive/MyDrive/RoBERTa - ARABIC - SEQUENCE\"\n","trainer.save_model(path)"]},{"cell_type":"markdown","metadata":{"id":"2XFsUz0pwN-b"},"source":["#### 2.3.5. Bengali"]},{"cell_type":"markdown","metadata":{"id":"P-WxgJR6xGCo"},"source":["##### 2.3.5.1. Pre-process (Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["b88afa81556d437e94fb70bc5321f5a2","8e174b0853ff4aefbd91ff7fc82aab78","1e931f1a4fb848f992cd7c7446b3a745","cd74c889de9241d69f77d9122dcc2d8e","8d149d2b07784bf588471afd07d029a2","7bac0fa6b6bf4d6791196c6c78886d3b","941cdf369a90483da15cfe33646455e4","89cd9b82dcfc4899af7ec50c1039702c","63bbffc0a06c4d4ea7b80c7e1412c596","c738554c050c46b38a64193227d55268","2f582d1580644eb5b5b6365dad099ca1","d5c3e9a32b8f47db9c545074fe32e13b","efc476c4be37438596a6041d127e05d7","e54ef6954d31493cbc700e3b390bec82","127381f5f5c347bfb6154ffcc0d2ccf8","3d7f08436c674e0b801b337447faddf6","7f1170daca9d4750adb4b6f2459a2207","6f9a9150082b4c9884afa30af157fc4a","6233b0bfcad648d88f520c8980087a52","536fc0b9d4ea41fd8866e57ab7b6ac09","e2fe63e620aa47528ddb03802bef0dd1","447325dfa04142ff92d7c5685fdd6ad0"]},"executionInfo":{"elapsed":5367,"status":"ok","timestamp":1698396080308,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-120},"id":"f9iXOXHq1Ayo","outputId":"dfad839e-8dbc-4d72-8e98-83ff463ae6ef"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b88afa81556d437e94fb70bc5321f5a2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5c3e9a32b8f47db9c545074fe32e13b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# run the tokenizer\n","tokenized_train_bengali = train_data[1][1].map(partial(get_train_features),\n","                                              batched = True,\n","remove_columns = train_data[1][1].column_names)\n","\n","tokenized_val_bengali = val_data[1][1].map(partial(get_train_features),\n","                                              batched = True,\n","remove_columns = val_data[1][1].column_names)"]},{"cell_type":"markdown","metadata":{"id":"IBInj3H_xKCc"},"source":["##### 2.3.5.2. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":829},"executionInfo":{"elapsed":1590670,"status":"ok","timestamp":1698397739163,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-120},"id":"ztm6qI481MA8","outputId":"e93fbae4-cc6f-4fec-d945-6f01e32b3083"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 5029\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1887\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1887' max='1887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1887/1887 26:26, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.066200</td>\n","      <td>0.041823</td>\n","      <td>0.461942</td>\n","      <td>0.385120</td>\n","      <td>0.420048</td>\n","      <td>0.986567</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.040800</td>\n","      <td>0.030354</td>\n","      <td>0.485523</td>\n","      <td>0.477024</td>\n","      <td>0.481236</td>\n","      <td>0.987875</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.029900</td>\n","      <td>0.029813</td>\n","      <td>0.466539</td>\n","      <td>0.533917</td>\n","      <td>0.497959</td>\n","      <td>0.988760</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 233\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/bengali_sequence/checkpoint-629\n","Configuration saved in /content/drive/MyDrive/bengali_sequence/checkpoint-629/config.json\n","Model weights saved in /content/drive/MyDrive/bengali_sequence/checkpoint-629/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-629/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-629/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 233\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/bengali_sequence/checkpoint-1258\n","Configuration saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1258/config.json\n","Model weights saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1258/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1258/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1258/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 233\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/bengali_sequence/checkpoint-1887\n","Configuration saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1887/config.json\n","Model weights saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1887/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1887/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/bengali_sequence/checkpoint-1887/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=1887, training_loss=0.041390248183921084, metrics={'train_runtime': 1590.5841, 'train_samples_per_second': 9.485, 'train_steps_per_second': 1.186, 'total_flos': 3942184168876032.0, 'train_loss': 0.041390248183921084, 'epoch': 3.0})"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# load trainer\n","trainer = Trainer(\n","    model=model,\n","    args = args,\n","    train_dataset=tokenized_train_bengali,\n","    eval_dataset=tokenized_val_bengali,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9259,"status":"ok","timestamp":1698397748392,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-120},"id":"WZ6yCXs51W1O","outputId":"fc53e3e2-79d8-49d3-bbdc-36bacf1e7857"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/RoBERTa - BENGALI - SEQUENCE\n","Configuration saved in /content/drive/MyDrive/RoBERTa - BENGALI - SEQUENCE/config.json\n","Model weights saved in /content/drive/MyDrive/RoBERTa - BENGALI - SEQUENCE/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/RoBERTa - BENGALI - SEQUENCE/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/RoBERTa - BENGALI - SEQUENCE/special_tokens_map.json\n"]}],"source":["path = \"/content/drive/MyDrive/RoBERTa - BENGALI - SEQUENCE\"\n","trainer.save_model(path)"]},{"cell_type":"markdown","metadata":{"id":"pFkHwAZfwQw6"},"source":["#### 2.3.6. Indonesian"]},{"cell_type":"markdown","metadata":{"id":"8CbwwrzrxN74"},"source":["##### 2.3.6.1. Pre-process (Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["b805755b083649e38c6009dd2deaae76","0346c5e0d0134b75b806fee4d7b8a5e7","443d44452a914490be72bb5af4242a4b","0d1e4e7874674aed91fc35dd3d857612","1565c6fc2f094fdf8fea27f694a3567b","51398174aacf4e7d86ea3dd1ab4208d6","8ac2740da2d841fea3665cec7a6205fa","0ba5db1248e94776ac295de3644b48cf","17f83fa4d97449c3b0dca4dc14290eed","6afb7ff247c74b449a2089e355356f8d","1fa80d10579d44e596315ff3d1a21ac0","d3a159f994ae4c3f89b2f12f65fa75e7","fa9ddb43c0524c68ae4ce26a385db630","47a0a30caace45618ef8aa08ba1db4b2","cea567d2209b42849ee34cf5472fcbdd","f673bf3684f84d21ab3a7499f32e5d35","a90597b34c9746f39111ae3d095a206f","3f1553aebaa440d8bda556277907b8f3","a1e78760403540268866112600f1c054","f68e8916f19941a7bf34418cd7b163ba","f544a88053054089909b676c0de77858","437fd21b46b74573a6ce01b986452f0a"]},"executionInfo":{"elapsed":12370,"status":"ok","timestamp":1698401646094,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-120},"id":"jOpZ6DiL1scz","outputId":"90a6ae33-ac44-47e4-917b-709d04949281"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b805755b083649e38c6009dd2deaae76","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3a159f994ae4c3f89b2f12f65fa75e7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# run the tokenizer\n","tokenized_train_indonesian = train_data[2][1].map(partial(get_train_features),\n","                                                  batched = True,\n","                                                  remove_columns = train_data[2][1].column_names)\n","\n","tokenized_val_indonesian = val_data[2][1].map(partial(get_train_features),\n","                                              batched = True,\n","                                              remove_columns = val_data[2][1].column_names)"]},{"cell_type":"markdown","metadata":{"id":"wADPItj8xSeO"},"source":["##### 2.3.6.2. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":950},"executionInfo":{"elapsed":216420,"status":"ok","timestamp":1698405385495,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-120},"id":"ihAmb7Ta19mV","outputId":"efb590da-ea4e-49c9-9dcd-2e33021743e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 11573\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4341\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4155' max='4341' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4155/4341 58:09 < 02:36, 1.19 it/s, Epoch 2.87/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.082100</td>\n","      <td>0.055658</td>\n","      <td>0.605953</td>\n","      <td>0.568570</td>\n","      <td>0.586667</td>\n","      <td>0.982357</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.052700</td>\n","      <td>0.045156</td>\n","      <td>0.633125</td>\n","      <td>0.674031</td>\n","      <td>0.652938</td>\n","      <td>0.983880</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1208\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/indonesian_sequence/checkpoint-1447\n","Configuration saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1447/config.json\n","Model weights saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1447/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1447/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-1447/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1208\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/indonesian_sequence/checkpoint-2894\n","Configuration saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2894/config.json\n","Model weights saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2894/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2894/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-2894/special_tokens_map.json\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4341' max='4341' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4341/4341 1:01:47, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.082100</td>\n","      <td>0.055658</td>\n","      <td>0.605953</td>\n","      <td>0.568570</td>\n","      <td>0.586667</td>\n","      <td>0.982357</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.052700</td>\n","      <td>0.045156</td>\n","      <td>0.633125</td>\n","      <td>0.674031</td>\n","      <td>0.652938</td>\n","      <td>0.983880</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.038400</td>\n","      <td>0.049386</td>\n","      <td>0.633982</td>\n","      <td>0.706128</td>\n","      <td>0.668113</td>\n","      <td>0.984025</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1208\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/indonesian_sequence/checkpoint-4341\n","Configuration saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4341/config.json\n","Model weights saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4341/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4341/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/indonesian_sequence/checkpoint-4341/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=4341, training_loss=0.05864648614658021, metrics={'train_runtime': 3711.3245, 'train_samples_per_second': 9.355, 'train_steps_per_second': 1.17, 'total_flos': 9071962097117184.0, 'train_loss': 0.05864648614658021, 'epoch': 3.0})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainer = Trainer(\n","    model=model,\n","    args = args,\n","    train_dataset=tokenized_train_indonesian,\n","    eval_dataset=tokenized_val_indonesian,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11260,"status":"ok","timestamp":1698405396738,"user":{"displayName":"Kaylie C","userId":"18064122393005532939"},"user_tz":-120},"id":"eqzmdoHi19mo","outputId":"a312cd28-23e3-4054-e4c8-99c77bd1a21b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/RoBERTa - INDONESIAN - SEQUENCE\n","Configuration saved in /content/drive/MyDrive/RoBERTa - INDONESIAN - SEQUENCE/config.json\n","Model weights saved in /content/drive/MyDrive/RoBERTa - INDONESIAN - SEQUENCE/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/RoBERTa - INDONESIAN - SEQUENCE/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/RoBERTa - INDONESIAN - SEQUENCE/special_tokens_map.json\n"]}],"source":["path = \"/content/drive/MyDrive/RoBERTa - INDONESIAN - SEQUENCE\"\n","trainer.save_model(path)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOWI6AQibl63xGrlJ4wDKTA","collapsed_sections":["0jDRMwJFG31h","S5n302rlIblM","wjs9ApFwMrog","VkrY-lgQMv-J","xQkFMJpJKTij","6x5mkiQsF5Pr","0j0Ej5YdF9aw","YcDiiknyGSh4","DfcOHHzdGx3x","LoS7HKejJ_-z","Ic1GISduKJin","DBxNdus5KQQR","NiEXQmrah5kP","GSet_7YziKTq","LNq2eDaziOQ-","xc1t4Xlsj535","g4B15yEfHxpp","jZpeJiBd-78G","N7ME7BUCOIIW","QxCa61UuP-ac","APJINxMTSAkA","3e-78-7rEuMN","v57_DXHKZRoj","VGWDZdX7mwl3","HsONQ7ejmwmY","Ef_x6CdMmwma","DuijSbhHmwmi","aLHO9NNgmwmm","oeaZ5XUBmwmq","5TEVGH_Zmwmt","TuasxVAcrErH","KW4p9BYNrEro","CuSeQrOgrErq","tjy_0ovQrEru","hrNaUbeKrErx","ngwzI3wCrEry","G99H4etZrEr2","CuVB6UcMGRaU","8YT6YiLkGXk0","pbikfKNmFIxF","qSdTgT8dGxkc","OWxzduO_I3Rm","17qS9eXhKRL_","AwAoaLMnKjqn","DaWcEL0RLAoO","JI-rGNRVLAos","4X4sw9CALAov","y_hO2B4NMe9k","dZyHkKT0Me-A","tSzJfQubMe-D","jD6cMPy-GYHb","Sn7jI0UrPwZ0","n9V7qyo9v6zn","1jD-qk__v3lf","iZRZ0ONnwKMp","0ZcOsiP9wNjy","_22hZ-8bxBRH","2XFsUz0pwN-b","P-WxgJR6xGCo","IBInj3H_xKCc","pFkHwAZfwQw6","wADPItj8xSeO"],"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00b9291d5bed45be873f6a9f2980f26c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f679e4ef454944d5ab1ffd8c75dcdaa1","placeholder":"​","style":"IPY_MODEL_cb69a52bd42e47ca923e66d1299cb2c6","value":" 12/12 [00:00&lt;00:00, 24.86ba/s]"}},"038e6f9ff0b94184a240c3892dc36f03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"039c629e89b84b518d66515073e201c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03c5d5f80b4e4c09bc968bfb3e95b608":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1438f2760f224ea98098222cb6bd392a","placeholder":"​","style":"IPY_MODEL_ba73b11b236b4601851a0e725bf56714","value":" 5/5 [00:05&lt;00:00,  1.04s/ba]"}},"04bd98cae15c4812b8e1fb5c1f5d31ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05c54fcea24e4034aab781e6b3b39169":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1fe0ac76f97a4f679abd972d35683873","IPY_MODEL_1ac6a25806554927b77d6fffe8ea7d54","IPY_MODEL_e1894cf92108468f83e138d5ecf64400"],"layout":"IPY_MODEL_bbfa9febde794f098f447b0018a5a87e"}},"064ba2960219489fba35c5211509bf53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63a4db1dfe534ac2a5bd880f1f2cd218","placeholder":"​","style":"IPY_MODEL_aae8e5671e9c41f6a675a6f62336d25c","value":"100%"}},"06570ef118e14ff39b3b483341c9e381":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0565ad0f0394d25915451e3fbece13d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_292a178d2faf4dff8e512713ffc204a8","value":2}},"06b0706c5ae84e9b9fab11c78f76feef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa17017b49874f6292ff44c1f8ef0a17","max":14,"min":0,"orientation":"horizontal","style":"IPY_MODEL_640826849f6047519936174824c61d1a","value":14}},"08d8822084964988ad59d7dca84dfe7d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0974d9a7591f4edbbb1fd7c829c1b74e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09a4646b0da04b8ca6a9ed667d35fd21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a55780e4b4242b994ec4c9ba3f0cfd8","placeholder":"​","style":"IPY_MODEL_46971b04e2984a7b9444156fdf521c78","value":" 1/1 [00:00&lt;00:00,  3.82ba/s]"}},"0d045cabdd98471a87ff5e79e1d71e44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f067d49ee2244cea970a89b90f89ac71","placeholder":"​","style":"IPY_MODEL_38a7d29f35a94ce99b9eabb5584929a5","value":" 117/117 [00:02&lt;00:00, 44.13ba/s]"}},"0db16ee410bf484582d59ef2a6c93b5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f1d1197a9c74a37933ec9d8300f50bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ca3729be8064bcca4c878ea81168a4f","IPY_MODEL_2f2428f74d274a1fb4547d7fd6212d3f","IPY_MODEL_9bde047ee13e4fe9942c47355771ff84"],"layout":"IPY_MODEL_8f2b44cf3c0d43849833abf9a27fef5d"}},"0f75cf95541a439081f6d3d4a863468c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58f00d9f2c8a442cbeafb03404686ac1","IPY_MODEL_71f7aa41391b4eb9a50183c3c3b36566","IPY_MODEL_00b9291d5bed45be873f6a9f2980f26c"],"layout":"IPY_MODEL_ce9b00b313624f648606d0c33ea4a7b6"}},"12e3a7a9a39542509e0742efc4242668":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8586d02549354dcd801dafee53d990ec","max":117,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04bd98cae15c4812b8e1fb5c1f5d31ce","value":117}},"1438f2760f224ea98098222cb6bd392a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15135de7cc36433c85af179adc9a56e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64ee4c6f73e34d97aa56d245fb7f7899","IPY_MODEL_c9dfc00401154bbb85803d22377008bc","IPY_MODEL_356d0104f03b4f0dbfcf9808f486c465"],"layout":"IPY_MODEL_8c90eb71612d466eac2f9059b9109b1b"}},"17b9232df19145ac84e2b48df651b3e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17dce79fcff84c3789d97b7a945879c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"181603bcf6de49aaa846bf42d397099f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ac6a25806554927b77d6fffe8ea7d54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_403025eac97b44f389bc4400ca6bd341","max":14,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e5e5bcd5f18d48a89f5b3bfbdabdd4ea","value":14}},"1f458fabd374489890ab070e501a5032":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fe0ac76f97a4f679abd972d35683873":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b99daa73b4843ff922b0a509f407b72","placeholder":"​","style":"IPY_MODEL_a3b526a7a638420b8199c45a92ec1922","value":"100%"}},"1ffbbb34680b49ca8b5f5b4f96e3c7d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24fe6bcd4f4d42b5bffee1c1855e9412":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43ff7e3c15df4a29a396f27640263f54","placeholder":"​","style":"IPY_MODEL_9a5cf6a1ba164d4587af1a20b72e3faa","value":"100%"}},"2907c39265904f30b9aaef8c5ee9f17a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"292a178d2faf4dff8e512713ffc204a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2aa675d9e6e54247aa3a8b0aab6d1038":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cdb3d1d94b048289505a36df714258b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aa675d9e6e54247aa3a8b0aab6d1038","placeholder":"​","style":"IPY_MODEL_b53f9ed7c49f44b6826ab1949f6dfb12","value":" 117/117 [00:04&lt;00:00, 36.19ba/s]"}},"2f2428f74d274a1fb4547d7fd6212d3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be6575530baa45a6a763ee19f6f24e24","max":117,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d9a144022fa44ac80296327fbd52cd3","value":117}},"3092df151e85404bbdc8dca2b6d1da4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5f9c65574264856a87c22aa816e2cfc","IPY_MODEL_38eda30294374c89abe2f04a9b54cbc4","IPY_MODEL_afd9ae41f8ee474dae610213816ec6ef"],"layout":"IPY_MODEL_32daa079d8bd427cad5c216221dea2a2"}},"32daa079d8bd427cad5c216221dea2a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34edcafe22e34eb2af01c2e6cc9d867c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89d1a01557684b4c979e1c0be2f4d66e","IPY_MODEL_06b0706c5ae84e9b9fab11c78f76feef","IPY_MODEL_85bb74f964e24cfda15d1b62add91055"],"layout":"IPY_MODEL_e500e986c1c8417dbcd941332f938f08"}},"356d0104f03b4f0dbfcf9808f486c465":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb462ae5264e491c81f5abe9990e91d7","placeholder":"​","style":"IPY_MODEL_7313a2cc67d341569dd184c4a174bc49","value":" 1191/1191 [06:22&lt;00:00,  3.15it/s]"}},"36d49492b00a444b992659f292913091":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3842f4c3e20b4a019b024a77fbe76e09","IPY_MODEL_d007b39fa5c5484f8982c6b3b3de4745","IPY_MODEL_b4d05d53bea145338e17b5576ee823cc"],"layout":"IPY_MODEL_5d3db5a5095c4af1a4ff3bb3b938fcb8"}},"37e1e1ef06634dd884c8253d480f42d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37e8d462f3c6443bb2eb34338beb194d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3842f4c3e20b4a019b024a77fbe76e09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fac6eeb96f834091ac9cb403d7926071","placeholder":"​","style":"IPY_MODEL_e0b9dd08e71d499891d710e3c08e67b0","value":"Flattening the indices: 100%"}},"38a7d29f35a94ce99b9eabb5584929a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38eda30294374c89abe2f04a9b54cbc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83e124e0d07749548907aaba8077c8cc","max":14,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c4a083274eb43b386b9a5e83ca22a04","value":14}},"39a4a46ad3414031b712bf490667593e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39db5e8c4e644f83874d1b9802079c6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cf623e487a342e5aade961e2041a45f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b23e2725e58491fbe8f50ef9cf4ac4c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e78a093c51c44a8cb8cc493155273fac","value":1}},"3f3d23d5ebfc4d8d9745d0a89179599f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08d8822084964988ad59d7dca84dfe7d","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c4bd2a0a1de4186b2d067d44bf031d1","value":5}},"3fc944cda1c04f54a165182576978d46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"403025eac97b44f389bc4400ca6bd341":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42bf5d36a1e44fc1a6d238f6fb54ab54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80e5452149f6470b9dbeaa0fcdaf9c6e","placeholder":"​","style":"IPY_MODEL_0974d9a7591f4edbbb1fd7c829c1b74e","value":"Flattening the indices: 100%"}},"43b8c5fdfeaf487b935aa8418697934e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e66218e3f3244c8a85b4f115d128651","placeholder":"​","style":"IPY_MODEL_9a876f8b221841a2bd0ce4232e315311","value":" 2/2 [00:00&lt;00:00, 23.60ba/s]"}},"43ff7e3c15df4a29a396f27640263f54":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"461cc3b85deb4dfeafb2a163c2296081":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46971b04e2984a7b9444156fdf521c78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4837dab577864530b30a25b69330f1e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"485c925610864aecb62225ecc60385fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5faf977e2b634cc0aaaddd44a0505c7c","max":14,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60802dd06c934ae49916eae72d225e12","value":14}},"493da70fad564dabae9c2bcc6a35f507":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4953ad6e0acd4679a287b120a284bd0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"497b9727367b482e81de5ea19ae5738e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a3d9e98f2144c6992a35cf991015678":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e61f1dd27ce4fad99d866159e51b733":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e66218e3f3244c8a85b4f115d128651":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fd5e212be3d4235a63b108310a3a061":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17b9232df19145ac84e2b48df651b3e1","placeholder":"​","style":"IPY_MODEL_f6806e57b2bc44a9acf4f151e1c2b78d","value":" 5/5 [00:00&lt;00:00, 12.39ba/s]"}},"50a5d49e45f04d39a899c8fcbb0d0e68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58f00d9f2c8a442cbeafb03404686ac1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37e8d462f3c6443bb2eb34338beb194d","placeholder":"​","style":"IPY_MODEL_37e1e1ef06634dd884c8253d480f42d7","value":"Flattening the indices: 100%"}},"59994fdbc3554bf7ab3730f1041944f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a55780e4b4242b994ec4c9ba3f0cfd8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b99daa73b4843ff922b0a509f407b72":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d3db5a5095c4af1a4ff3bb3b938fcb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5faf977e2b634cc0aaaddd44a0505c7c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60802dd06c934ae49916eae72d225e12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60c490263ed743b796082452c7b285af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e522fa3e45ec42aa9be11cf86209c147","placeholder":"​","style":"IPY_MODEL_eed3d617fa4b43858e7f2fa645e22243","value":"100%"}},"6243a2d61dd0489a829245806dc40c15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"630efe85b88340da8019991936e1b044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60c490263ed743b796082452c7b285af","IPY_MODEL_12e3a7a9a39542509e0742efc4242668","IPY_MODEL_0d045cabdd98471a87ff5e79e1d71e44"],"layout":"IPY_MODEL_39a4a46ad3414031b712bf490667593e"}},"63a4db1dfe534ac2a5bd880f1f2cd218":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63ed01bdc4e84303a3caa9031df442a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"640826849f6047519936174824c61d1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"642be3bcd99146f597b927f65e9e6306":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_181603bcf6de49aaa846bf42d397099f","placeholder":"​","style":"IPY_MODEL_cc5d9d95271c4ffd96632f4521f63206","value":"100%"}},"64ee4c6f73e34d97aa56d245fb7f7899":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fc944cda1c04f54a165182576978d46","placeholder":"​","style":"IPY_MODEL_bd2053ba16524715b48ff27694abc9f6","value":"100%"}},"6b23e2725e58491fbe8f50ef9cf4ac4c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c4a083274eb43b386b9a5e83ca22a04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6cc7a8b0b695471aafddbef08ca98935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fdef91fa51a47fb9375cab9b8ece79a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"711d184b921245d188a6c38b88340bd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cc7a8b0b695471aafddbef08ca98935","placeholder":"​","style":"IPY_MODEL_be4636e01b6342f3afaa110d036448bc","value":" 117/117 [00:03&lt;00:00, 46.83ba/s]"}},"71f7aa41391b4eb9a50183c3c3b36566":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_038e6f9ff0b94184a240c3892dc36f03","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e61f1dd27ce4fad99d866159e51b733","value":12}},"7313a2cc67d341569dd184c4a174bc49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74da72c0186f40c6a296c999382a5ca1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24fe6bcd4f4d42b5bffee1c1855e9412","IPY_MODEL_3cf623e487a342e5aade961e2041a45f","IPY_MODEL_09a4646b0da04b8ca6a9ed667d35fd21"],"layout":"IPY_MODEL_b9ac04fd8b7244ad9d11360004dd0db1"}},"7626c601c41540bba989bc9b9ffb838f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78f2e53af6c147ee8be5ea557f590c27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78f3b55a15414c01ae77d647a0bf286e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c4bd2a0a1de4186b2d067d44bf031d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d9a144022fa44ac80296327fbd52cd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e8adfda96804338b735ff68b0af21af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59994fdbc3554bf7ab3730f1041944f6","max":117,"min":0,"orientation":"horizontal","style":"IPY_MODEL_497b9727367b482e81de5ea19ae5738e","value":117}},"80e5452149f6470b9dbeaa0fcdaf9c6e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83e124e0d07749548907aaba8077c8cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84d25daeda16413b92c0443382659c03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f571e64c70d418e8f3556581ec178a4","IPY_MODEL_e5ff0d5b84a24f46b501734d4248e9f0","IPY_MODEL_2cdb3d1d94b048289505a36df714258b"],"layout":"IPY_MODEL_4a3d9e98f2144c6992a35cf991015678"}},"85337737701d4c3298b9d40f347f9308":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8586d02549354dcd801dafee53d990ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85bb74f964e24cfda15d1b62add91055":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99e185f909db4849ab8e5eb489e585c3","placeholder":"​","style":"IPY_MODEL_f754e7ad93b44a62b036718f9b97d0d8","value":" 14/14 [00:00&lt;00:00, 40.35ba/s]"}},"89d1a01557684b4c979e1c0be2f4d66e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f458fabd374489890ab070e501a5032","placeholder":"​","style":"IPY_MODEL_afce79b98ba4415cbd0044bc21d88874","value":"100%"}},"8a51bd7900ec49a48f907bc17096e484":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85337737701d4c3298b9d40f347f9308","placeholder":"​","style":"IPY_MODEL_bfe6a33019ca41c6ad163bd8e109ca2c","value":" 14/14 [00:00&lt;00:00, 42.24ba/s]"}},"8c90eb71612d466eac2f9059b9109b1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d6bb5ea380e4734acab29994315fd56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f2b44cf3c0d43849833abf9a27fef5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f571e64c70d418e8f3556581ec178a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcc7dfa675fa48f38dbdb8d7af748d64","placeholder":"​","style":"IPY_MODEL_039c629e89b84b518d66515073e201c7","value":"100%"}},"97139e555b4040358bae623739f5dfe9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99e185f909db4849ab8e5eb489e585c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a5cf6a1ba164d4587af1a20b72e3faa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a876f8b221841a2bd0ce4232e315311":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bde047ee13e4fe9942c47355771ff84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f2e53af6c147ee8be5ea557f590c27","placeholder":"​","style":"IPY_MODEL_17dce79fcff84c3789d97b7a945879c8","value":" 117/117 [00:02&lt;00:00, 43.75ba/s]"}},"9ca3729be8064bcca4c878ea81168a4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae8596b7e4b34f54876e393dce15d876","placeholder":"​","style":"IPY_MODEL_97139e555b4040358bae623739f5dfe9","value":"100%"}},"a3b526a7a638420b8199c45a92ec1922":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a964da1208f8419f87295f283ae1e105":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e072f62986814584b76c1fc3cdca4a93","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6243a2d61dd0489a829245806dc40c15","value":5}},"aa17017b49874f6292ff44c1f8ef0a17":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aae8e5671e9c41f6a675a6f62336d25c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac8a0ca4001f4f0392c29116b33e5a6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_642be3bcd99146f597b927f65e9e6306","IPY_MODEL_485c925610864aecb62225ecc60385fb","IPY_MODEL_8a51bd7900ec49a48f907bc17096e484"],"layout":"IPY_MODEL_6fdef91fa51a47fb9375cab9b8ece79a"}},"ade28c7f982e48b5bc67e831c6e53595":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae8596b7e4b34f54876e393dce15d876":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afce79b98ba4415cbd0044bc21d88874":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afd9ae41f8ee474dae610213816ec6ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f3b55a15414c01ae77d647a0bf286e","placeholder":"​","style":"IPY_MODEL_dfded47221dc49e2abad91c03e308201","value":" 14/14 [00:00&lt;00:00, 41.92ba/s]"}},"b0509d277ecb44c0bf0d14b3cf38397a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63ed01bdc4e84303a3caa9031df442a7","placeholder":"​","style":"IPY_MODEL_c69b3546a09f439cb22fe0c2c41ebd8e","value":"Flattening the indices: 100%"}},"b4d05d53bea145338e17b5576ee823cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_461cc3b85deb4dfeafb2a163c2296081","placeholder":"​","style":"IPY_MODEL_2907c39265904f30b9aaef8c5ee9f17a","value":" 1/1 [00:00&lt;00:00, 19.91ba/s]"}},"b53f9ed7c49f44b6826ab1949f6dfb12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b95a42a3b208403ea2c17e1cb2d853e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42bf5d36a1e44fc1a6d238f6fb54ab54","IPY_MODEL_06570ef118e14ff39b3b483341c9e381","IPY_MODEL_43b8c5fdfeaf487b935aa8418697934e"],"layout":"IPY_MODEL_cbb7e4afe90e48108d74d5749ca0d7d3"}},"b9ac04fd8b7244ad9d11360004dd0db1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba73b11b236b4601851a0e725bf56714":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbfa9febde794f098f447b0018a5a87e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd2053ba16524715b48ff27694abc9f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be4636e01b6342f3afaa110d036448bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be6575530baa45a6a763ee19f6f24e24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfe6a33019ca41c6ad163bd8e109ca2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c11e83aa55a14fe5bc18d3505d9cb1c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5f4a82586104e988dbf59179b6ca941":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5f9c65574264856a87c22aa816e2cfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf630330577a44a09624aca291958c46","placeholder":"​","style":"IPY_MODEL_4837dab577864530b30a25b69330f1e9","value":"100%"}},"c69b3546a09f439cb22fe0c2c41ebd8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9dfc00401154bbb85803d22377008bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ade28c7f982e48b5bc67e831c6e53595","max":1191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0db16ee410bf484582d59ef2a6c93b5b","value":1191}},"cb69a52bd42e47ca923e66d1299cb2c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbb7e4afe90e48108d74d5749ca0d7d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc5d9d95271c4ffd96632f4521f63206":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd7740cbe2bd446283ff95cd26877916":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce9b00b313624f648606d0c33ea4a7b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf630330577a44a09624aca291958c46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d007b39fa5c5484f8982c6b3b3de4745":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f4a82586104e988dbf59179b6ca941","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd7740cbe2bd446283ff95cd26877916","value":1}},"dfd32dbf334f4685bdc4a55df475b3ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfded47221dc49e2abad91c03e308201":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0565ad0f0394d25915451e3fbece13d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e072f62986814584b76c1fc3cdca4a93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0b9dd08e71d499891d710e3c08e67b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1894cf92108468f83e138d5ecf64400":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ffbbb34680b49ca8b5f5b4f96e3c7d8","placeholder":"​","style":"IPY_MODEL_39db5e8c4e644f83874d1b9802079c6f","value":" 14/14 [00:00&lt;00:00, 40.51ba/s]"}},"e500e986c1c8417dbcd941332f938f08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e522fa3e45ec42aa9be11cf86209c147":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5e5bcd5f18d48a89f5b3bfbdabdd4ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5ff0d5b84a24f46b501734d4248e9f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c11e83aa55a14fe5bc18d3505d9cb1c5","max":117,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfd32dbf334f4685bdc4a55df475b3ce","value":117}},"e78a093c51c44a8cb8cc493155273fac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8279088aa2e492b80149a0e42263917":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea7727cc19a5486a8b6f6bdef3900f6b","IPY_MODEL_3f3d23d5ebfc4d8d9745d0a89179599f","IPY_MODEL_03c5d5f80b4e4c09bc968bfb3e95b608"],"layout":"IPY_MODEL_50a5d49e45f04d39a899c8fcbb0d0e68"}},"ea7727cc19a5486a8b6f6bdef3900f6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_493da70fad564dabae9c2bcc6a35f507","placeholder":"​","style":"IPY_MODEL_4953ad6e0acd4679a287b120a284bd0c","value":"100%"}},"eed3d617fa4b43858e7f2fa645e22243":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f067d49ee2244cea970a89b90f89ac71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5196c975d844e1a8368eb049ae80e72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_064ba2960219489fba35c5211509bf53","IPY_MODEL_7e8adfda96804338b735ff68b0af21af","IPY_MODEL_711d184b921245d188a6c38b88340bd6"],"layout":"IPY_MODEL_7626c601c41540bba989bc9b9ffb838f"}},"f548ad2755cb44b082fe5b73dbd22d53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0509d277ecb44c0bf0d14b3cf38397a","IPY_MODEL_a964da1208f8419f87295f283ae1e105","IPY_MODEL_4fd5e212be3d4235a63b108310a3a061"],"layout":"IPY_MODEL_8d6bb5ea380e4734acab29994315fd56"}},"f679e4ef454944d5ab1ffd8c75dcdaa1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6806e57b2bc44a9acf4f151e1c2b78d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f754e7ad93b44a62b036718f9b97d0d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fac6eeb96f834091ac9cb403d7926071":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb462ae5264e491c81f5abe9990e91d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcc7dfa675fa48f38dbdb8d7af748d64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
