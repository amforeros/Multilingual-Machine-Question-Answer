{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["0jDRMwJFG31h","S5n302rlIblM","wjs9ApFwMrog","VkrY-lgQMv-J","zrOaTX15Ie8s","xQkFMJpJKTij","r9x_tTJNKZ-z","VUDVf7zBaL-S","6rQaD28naPxb","4I7Wv-r9elQj","FfWubJD7iIYZ"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Week 37 - Language Models"],"metadata":{"id":"XOwOS12oHJrx"}},{"cell_type":"markdown","source":["## 1. Setup"],"metadata":{"id":"0jDRMwJFG31h"}},{"cell_type":"markdown","source":["### 1.1. Libraries"],"metadata":{"id":"S5n302rlIblM"}},{"cell_type":"markdown","source":["#### 1.1.1. New Libraries"],"metadata":{"id":"wjs9ApFwMrog"}},{"cell_type":"code","source":["# new libraries for Google Colab\n","!pip3 install nltk\n","!pip3 install datasets\n","!pip install bnlp-toolkit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z08V_ZPyIecS","executionInfo":{"status":"ok","timestamp":1698920820216,"user_tz":-60,"elapsed":12092,"user":{"displayName":"Alex YE","userId":"10586116328910308711"}},"outputId":"704499f4-d161-4fe8-af89-14e6aa18fd21"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: bnlp-toolkit in /usr/local/lib/python3.10/dist-packages (4.0.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (0.1.99)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (4.3.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (1.11.3)\n","Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (0.3.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (4.66.1)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (6.1.1)\n","Requirement already satisfied: emoji==1.7.0 in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (1.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit) (2.31.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->bnlp-toolkit) (0.2.8)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bnlp-toolkit) (6.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit) (2023.6.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit) (2023.7.22)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit) (0.9.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit) (1.16.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit) (0.9.0)\n"]}]},{"cell_type":"markdown","source":["#### 1.1.2. Load Libraries"],"metadata":{"id":"VkrY-lgQMv-J"}},{"cell_type":"code","source":["from datasets import load_dataset                       # library to import data from huggingface\n","import pandas as pd                                     # library to transform to dataframe. helps for statistics\n","from bnlp import BasicTokenizer                         # library for bengali tokenizer\n","from nltk.tokenize import word_tokenize                 # library for tokenize arabic and indonesian\n","import numpy as np                                      # library for math operations and matrices\n","import seaborn as sns                                   # library for making plots\n","from scipy.stats.mstats import gmean                    # libreary for geometric mean and perplexity"],"metadata":{"id":"nXG9jTstJqQZ","executionInfo":{"status":"ok","timestamp":1698920820217,"user_tz":-60,"elapsed":8,"user":{"displayName":"Alex YE","userId":"10586116328910308711"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 1.2. Data"],"metadata":{"id":"zrOaTX15Ie8s"}},{"cell_type":"markdown","source":["#### 1.2.1. Read Data"],"metadata":{"id":"xQkFMJpJKTij"}},{"cell_type":"code","source":["# load training dataset\n","datasets_train = load_dataset(\"copenlu/answerable_tydiqa\", split='train')\n","# load validation dataset\n","datasets_val = load_dataset(\"copenlu/answerable_tydiqa\", split='validation')"],"metadata":{"id":"co_f3SN1J8FN","executionInfo":{"status":"ok","timestamp":1698920822325,"user_tz":-60,"elapsed":2114,"user":{"displayName":"Alex YE","userId":"10586116328910308711"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Bengali Tolkenizer\n","bengali_tokenizer = BasicTokenizer()"],"metadata":{"id":"jBxhwzwDNaQZ","executionInfo":{"status":"ok","timestamp":1698920822326,"user_tz":-60,"elapsed":8,"user":{"displayName":"Alex YE","userId":"10586116328910308711"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#### 1.2.2. Transform Data"],"metadata":{"id":"r9x_tTJNKZ-z"}},{"cell_type":"code","source":["# define languages for the project\n","languages = ['arabic', 'bengali','indonesian']\n","\n","# transform to pandas dataframe\n","pandas_datasets_train = pd.DataFrame(datasets_train)\n","pandas_datasets_val = pd.DataFrame(datasets_val)\n","\n","# filter languaje\n","df_train_filter = pandas_datasets_train[pandas_datasets_train['language'].isin(languages)]\n","df_val_filter = pandas_datasets_val[pandas_datasets_val['language'].isin(languages)]"],"metadata":{"id":"wm-PFWi_KZYC","executionInfo":{"status":"ok","timestamp":1698920834346,"user_tz":-60,"elapsed":12026,"user":{"displayName":"Alex YE","userId":"10586116328910308711"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# tokens documents\n","df_train_filter['tokens_document'] = np.where(df_train_filter['language'].isin(['indonesian','arabic']),\n","         df_train_filter.document_plaintext.apply(lambda x: word_tokenize(x)),\n","         df_train_filter.document_plaintext.apply(lambda x: bengali_tokenizer(x)))\n","df_train_filter['tokens_document_len'] = df_train_filter['tokens_document'].apply(lambda x: len(x))\n","\n","df_val_filter['tokens_document'] = np.where(df_val_filter['language'].isin(['indonesian','arabic']),\n","         df_val_filter.document_plaintext.apply(lambda x: word_tokenize(x)),\n","         df_val_filter.document_plaintext.apply(lambda x: bengali_tokenizer(x)))\n","df_val_filter['tokens_document_len'] = df_val_filter['tokens_document'].apply(lambda x: len(x))\n","\n","\n","# tokens question\n","df_train_filter['tokens_question'] = np.where(df_train_filter['language'].isin(['indonesian','arabic']),\n","         df_train_filter.question_text.apply(lambda x: word_tokenize(x)),\n","         df_train_filter.question_text.apply(lambda x: bengali_tokenizer(x)))\n","df_train_filter['tokens_question_len'] = df_train_filter['tokens_question'].apply(lambda x: len(x))\n","\n","df_val_filter['tokens_question'] = np.where(df_val_filter['language'].isin(['indonesian','arabic']),\n","         df_val_filter.question_text.apply(lambda x: word_tokenize(x)),\n","         df_val_filter.question_text.apply(lambda x: bengali_tokenizer(x)))\n","df_val_filter['tokens_question_len'] = df_val_filter['tokens_question'].apply(lambda x: len(x))\n","\n","\n","# tokens answer\n","df_train_filter['tokens_answer'] = np.where(df_train_filter['language'].isin(['indonesian','arabic']),\n","         df_train_filter['annotations'].apply(lambda x: x['answer_text'][0]).apply(lambda x: word_tokenize(x)),\n","         df_train_filter['annotations'].apply(lambda x: x['answer_text'][0]).apply(lambda x: bengali_tokenizer(x)))\n","df_train_filter['tokens_answer_len'] = df_train_filter['tokens_answer'].apply(lambda x: len(x))\n","\n","df_val_filter['tokens_answer'] = np.where(df_val_filter['language'].isin(['indonesian','arabic']),\n","         df_val_filter['annotations'].apply(lambda x: x['answer_text'][0]).apply(lambda x: word_tokenize(x)),\n","         df_val_filter['annotations'].apply(lambda x: x['answer_text'][0]).apply(lambda x: bengali_tokenizer(x)))\n","df_val_filter['tokens_answer_len'] = df_val_filter['tokens_answer'].apply(lambda x: len(x))\n","\n","\n","# Insert START-OF-SENTENCE [SOS] AND END-OF-SENTENCE [EOS] for the language model\n","df_train_filter['tokens_document_endsentence'] = df_train_filter['tokens_document'].apply(lambda x: [\"[SOS]\"] + x + [\"[EOS]\"])\n","df_train_filter['tokens_question_endsentence'] = df_train_filter['tokens_question'].apply(lambda x: [\"[SOS]\"] + x + [\"[EOS]\"])\n","\n","df_val_filter['tokens_document_endsentence'] = df_val_filter['tokens_document'].apply(lambda x: [\"[SOS]\"] + x + [\"[EOS]\"])\n","df_val_filter['tokens_question_endsentence'] = df_val_filter['tokens_question'].apply(lambda x: [\"[SOS]\"] + x + [\"[EOS]\"])"],"metadata":{"id":"-PIR2ip8bwPT","executionInfo":{"status":"ok","timestamp":1698920884096,"user_tz":-60,"elapsed":49755,"user":{"displayName":"Alex YE","userId":"10586116328910308711"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## 2. Language Models"],"metadata":{"id":"VUDVf7zBaL-S"}},{"cell_type":"markdown","source":["### 2.1. Uniform"],"metadata":{"id":"6rQaD28naPxb"}},{"cell_type":"code","source":["def function_count_words(list_tokens = []):\n","  \"\"\"\n","  Funtion that counts frecuency of words in a list\n","  \"\"\"\n","  token_count = {}\n","  for token_i in list_tokens:\n","    if token_i in token_count.keys():\n","      token_count[token_i] += 1\n","    else:\n","      token_count[token_i] = 1\n","  return token_count"],"metadata":{"id":"8OB6Gbyhdd6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def replace_OOV(frequency_dict:{}, frequency_bottom =5 ):\n","  \"\"\"\n","  Words Out Of Vocabulary\n","  Replace Out Of Vocabulary ('[OOV]') for frequencies less than frequency_bottom\n","  \"\"\"\n","  updated_vocabulary_question = {}\n","  count_oov_times = 0\n","\n","  for word in frequency_dict.keys():\n","    if frequency_dict[word] >= frequency_bottom:\n","      updated_vocabulary_question[word] = frequency_dict[word]\n","    else:\n","      count_oov_times += frequency_dict[word]\n","      updated_vocabulary_question['[OOV]'] = count_oov_times\n","\n","  return updated_vocabulary_question"],"metadata":{"id":"0JtGoZi6dSY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_uniform(dict_vocabulary = {}):\n","  \"\"\"\n","  Define language uniform model\n","  \"\"\"\n","  size_vocab_questions = len(dict_vocabulary.keys())\n","  probability = {}\n","  for word_ in dict_vocabulary.keys():\n","    # in an uniform language model, every token has the same probability\n","    probability[word_] = (1 /size_vocab_questions, dict_vocabulary[word_])\n","  return  probability\n","\n","for language in languages:\n","  print(\"#\"*50)\n","  print(language)\n","  print(\"#\"*50)\n","  # documents\n","  sentences_document_train = df_train_filter[df_train_filter['language']==language]['tokens_document_endsentence']\n","  list_documents_tokens_train = sentences_document_train.to_list()\n","  flat_document_tokens_train = ([item for sublist in list_documents_tokens_train for item in sublist])\n","  print('Length Train Vocabulary Document',len(set(flat_document_tokens_train)))\n","\n","  # questions\n","  sentences_questions_train = df_train_filter[df_train_filter['language']==language]['tokens_question_endsentence']\n","  list_questions_tokens_train = sentences_questions_train.to_list()\n","  flat_questions_tokens_train = ([item for sublist in list_questions_tokens_train for item in sublist])\n","  print('Length Train Vocabulary Questions',len(set(flat_questions_tokens_train)))\n","\n","  # frequency\n","  counts_documents_train = function_count_words(flat_document_tokens_train)\n","  print('Length Train Vocabulary Document',len(counts_documents_train))\n","\n","  counts_questions_train = function_count_words(flat_questions_tokens_train)\n","  print('Length Train Vocabulary Question',len(counts_questions_train))\n","\n","  # replace low frequency with OOV\n","  updated_vocabulary_documents_train = replace_OOV(counts_documents_train)\n","  print('Length Train Vocabulary Document Using OOV',len(set(updated_vocabulary_documents_train)))\n","\n","\n","  updated_vocabulary_question_train = replace_OOV(counts_questions_train)\n","  print('Length Train Vocabulary Document Using OOV',len(set(updated_vocabulary_question_train)))\n","\n","  # model (Train)\n","  word_distribution_uniform_documents = model_uniform(updated_vocabulary_documents_train)\n","  word_distribution_uniform_questions = model_uniform(updated_vocabulary_question_train)\n","\n","  # Example of a sample uniform model\n","  sample_question_size_10 =[]\n","  sample_document_size_10 =[]\n","  for i in range(0,10):\n","    # random sample (10) according to the probability (Uniform)\n","    sample_document_i = np.random.choice(list(word_distribution_uniform_documents.keys()),\n","                  p=[x[0] for x in list(word_distribution_uniform_documents.values())])\n","    sample_question_i = np.random.choice(list(word_distribution_uniform_questions.keys()),\n","                  p=[x[0] for x in list(word_distribution_uniform_questions.values())])\n","    # append to the list\n","    sample_document_size_10.append(sample_document_i)\n","    sample_question_size_10.append(sample_question_i)\n","\n","\n","  print(f'Sample documents:',' '.join(sample_document_size_10))\n","  print(f'Sample questions:',' '.join(sample_question_size_10))\n","  print(\"-\"*50)\n","  # documents\n","  sentences_document_val = df_val_filter[df_val_filter['language']==language]['tokens_document_endsentence']\n","  list_documents_tokens_val = sentences_document_val.to_list()\n","  flat_document_tokens_val = ([item for sublist in list_documents_tokens_val for item in sublist])\n","  print('Length Test Vocabulary Document',len(set(flat_document_tokens_val)))\n","\n","  # questions\n","  sentences_questions_val = df_val_filter[df_val_filter['language']==language]['tokens_question_endsentence']\n","  list_questions_tokens_val = sentences_questions_val.to_list()\n","  flat_questions_tokens_val = ([item for sublist in list_questions_tokens_val for item in sublist])\n","  print('Length Test Vocabulary Question',len(set(flat_questions_tokens_val)))\n","\n","  # Replace OOV\n","  updated_vocabulary_documents_val = ([word if word in word_distribution_uniform_documents.keys() else '[OOV]' for word in flat_document_tokens_val])\n","  print('Length Test Sequence Document Using OOV',len((updated_vocabulary_documents_val)))\n","\n","\n","  updated_vocabulary_questions_val = ([word if word in word_distribution_uniform_questions.keys() else '[OOV]' for word in flat_questions_tokens_val])\n","  print('Length Test Sequence Questions Using OOV',len((updated_vocabulary_questions_val)))\n","\n","  # Perplexity\n","  prob_documents_val = []\n","  for word in updated_vocabulary_documents_val:\n","    touple_observed = (word, word_distribution_uniform_documents[word][0])\n","    prob_documents_val.append(touple_observed)\n","  # tokens apear many times\n","  #prob_documents_val = set(prob_documents_val)\n","  print('Perplexity documents:', np.round(gmean([1/x for x in [y for x,y in prob_documents_val] ])))\n","\n","  prob_questions_val = []\n","  for word in updated_vocabulary_questions_val:\n","    touple_observed = (word, word_distribution_uniform_questions[word][0])\n","    prob_questions_val.append(touple_observed)\n","  # tokens apear many times\n","  #prob_documents_val = set(prob_documents_val)\n","  print('Perplexity questions:', np.round(gmean([1/x for x in [y for x,y in prob_questions_val] ])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSw-zxfP8PqO","executionInfo":{"status":"ok","timestamp":1698500724147,"user_tz":-120,"elapsed":4253,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"}},"outputId":"1dc2fb79-1652-41fe-9d4e-1072a663da3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##################################################\n","arabic\n","##################################################\n","Length Train Vocabulary Document 225191\n","Length Train Vocabulary Questions 16361\n","Length Train Vocabulary Document 225191\n","Length Train Vocabulary Question 16361\n","Length Train Vocabulary Document Using OOV 46238\n","Length Train Vocabulary Document Using OOV 3733\n","Sample documents: المذكرات وحضور ومغامرات حرجة البورمان لتصبح ماريّ خروجه مترين رفض\n","Sample questions: غير مخترع حروب تعمل الاقتصادية؟ المتجمد الكيميائي ديكارت المحافظين الأب\n","--------------------------------------------------\n","Length Test Vocabulary Document 37483\n","Length Test Vocabulary Question 2274\n","Length Test Sequence Document Using OOV 155827\n","Length Test Sequence Questions Using OOV 14554\n","Perplexity documents: 46238.0\n","Perplexity questions: 3733.0\n","##################################################\n","bengali\n","##################################################\n","Length Train Vocabulary Document 50094\n","Length Train Vocabulary Questions 3759\n","Length Train Vocabulary Document 50094\n","Length Train Vocabulary Question 3759\n","Length Train Vocabulary Document Using OOV 10193\n","Length Train Vocabulary Document Using OOV 1073\n","Sample documents: একথাও বোমার কলেজিয়েট সত্যার্থী বাণী মেমরি লক্ষণ ইব্রাহিম ৫৮১ বি.এ\n","Sample questions: সিরিজ মালয়েশিয়ার বকর মহকুমা সুলতানের অনুবাদক ইংল্যান্ড পতাকার প্রশাসনিক সাম্রাজ্যের\n","--------------------------------------------------\n","Length Test Vocabulary Document 6727\n","Length Test Vocabulary Question 438\n","Length Test Sequence Document Using OOV 22687\n","Length Test Sequence Questions Using OOV 2368\n","Perplexity documents: 10193.0\n","Perplexity questions: 1073.0\n","##################################################\n","indonesian\n","##################################################\n","Length Train Vocabulary Document 72405\n","Length Train Vocabulary Questions 5774\n","Length Train Vocabulary Document 72405\n","Length Train Vocabulary Question 5774\n","Length Train Vocabulary Document Using OOV 15803\n","Length Train Vocabulary Document Using OOV 1415\n","Sample documents: sekutu-sekutu menciptakan Pelaksana Lintang 74 Kirito Makiyyah diagram Rupert Arafat\n","Sample questions: pemerintah '' Bintang Santo Tentang van kepala Star asal Petrus\n","--------------------------------------------------\n","Length Test Vocabulary Document 18217\n","Length Test Vocabulary Question 1397\n","Length Test Sequence Document Using OOV 103005\n","Length Test Sequence Questions Using OOV 10088\n","Perplexity documents: 15803.0\n","Perplexity questions: 1415.0\n"]}]},{"cell_type":"markdown","source":["### 2.2. Unigram"],"metadata":{"id":"4I7Wv-r9elQj"}},{"cell_type":"code","source":["k_smoothing = 1\n","# Define unigram model with Laplace Smoothing\n","def model_unigram(dict_vocabulary={}, k=k_smoothing):\n","    total_words = sum(dict_vocabulary.values())\n","    V = len(dict_vocabulary)  # Vocabulary size\n","    probability = {}\n","    for word_, count in dict_vocabulary.items():\n","        probability[word_] = (count + k) / (total_words + k*V)\n","    return probability\n","\n","# Sample\n","def generate_sample_unigrams(distribution, n=10):\n","    words, probs = zip(*[(word, prob) for word, prob in distribution.items()])\n","    words = np.array(words)\n","    probs = np.array(probs) / sum(probs)\n","    sample_unigrams = np.random.choice(words, size=n, p=probs)\n","    return sample_unigrams"],"metadata":{"id":"H9aoi4cOiGqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["languages = ['arabic', 'bengali', 'indonesian']\n","\n","for language in languages:\n","    print(\"#\"*50)\n","    print(language)\n","    print(\"#\"*50)\n","\n","    # Training\n","    sentences_doc_train = df_train_filter[df_train_filter['language'] == language]['tokens_document_endsentence']\n","    sentences_ques_train = df_train_filter[df_train_filter['language'] == language]['tokens_question_endsentence']\n","\n","    flat_doc_tokens_train = [item for sublist in sentences_doc_train for item in sublist]\n","    flat_ques_tokens_train = [item for sublist in sentences_ques_train for item in sublist]\n","\n","    counts_documents_train = function_count_words(flat_doc_tokens_train)\n","    counts_questions_train = function_count_words(flat_ques_tokens_train)\n","\n","    #updated_vocab_docs_train = replace_OOV(counts_documents_train)\n","    #updated_vocab_ques_train = replace_OOV(counts_questions_train)\n","\n","    word_distribution_docs = model_unigram(counts_documents_train)\n","    word_distribution_ques = model_unigram(counts_questions_train)\n","\n","    sample_unigram_docs = generate_sample_unigrams(word_distribution_docs)\n","    sample_unigram_ques = generate_sample_unigrams(word_distribution_ques)\n","\n","    # Validation\n","    sentences_doc_val = df_val_filter[df_val_filter['language'] == language]['tokens_document_endsentence']\n","    sentences_ques_val = df_val_filter[df_val_filter['language'] == language]['tokens_question_endsentence']\n","\n","\n","\n","\n","    flat_doc_tokens_val = [item for sublist in sentences_doc_val for item in sublist]\n","    flat_ques_tokens_val = [item for sublist in sentences_ques_val for item in sublist]\n","\n","    print(f'Total Words Document Test {language}',len(flat_doc_tokens_val))\n","    print(f'Total Vocabulary Document Test {language}',len(set(flat_doc_tokens_val)))\n","\n","    print(f'Total Words Questions Test {language}',len(flat_ques_tokens_val))\n","    print(f'Total Vocabulary Questions Test {language}',len(set(flat_ques_tokens_val)))\n","\n","\n","    count = 0\n","    # smooth prob for tokens OOV (document)\n","    V_size_docs = len(word_distribution_docs)\n","    total_words_documents = np.sum(list(counts_documents_train.values()))\n","    min_prob_docs =(count + k_smoothing) / (total_words_documents + k_smoothing*V_size_docs)\n","    print(f'Total Words Document Training {language}',total_words_documents)\n","    print(f'Total Vocabulary Document Training {language}',V_size_docs)\n","\n","\n","    # smooth prob for tokens OOV (question)\n","    V_size_ques = len(word_distribution_ques)\n","    total_words_ques = np.sum(list(counts_questions_train.values()))\n","    min_prob_ques = (count + k_smoothing) / (total_words_ques + k_smoothing*V_size_ques)\n","    print(f'Total Words Question Training {language}',total_words_ques)\n","    print(f'Total Vocabulary Question Training {language}',V_size_ques)\n","\n","    word_prob_documents_val = []\n","    for word_document_val_i in flat_doc_tokens_val:\n","        # If the token is in the training, assing prob of training\n","        if word_document_val_i in word_distribution_docs.keys():\n","          prob_i = word_distribution_docs.get(word_document_val_i)\n","        # if the token is not in the training, assing pro of smooth\n","        else:\n","          prob_i = min_prob_docs\n","        # save\n","        word_prob_documents_val.append(prob_i)\n","\n","    word_prob_question_val = []\n","    for word_question_val_i in flat_ques_tokens_val:\n","        # If the token is in the training, assing prob of training\n","        if word_question_val_i in word_distribution_ques.keys():\n","          prob_i = word_distribution_ques.get(word_question_val_i)\n","        # if the token is not in the training, assing pro of smooth\n","        else:\n","          prob_i = min_prob_ques\n","        # save\n","        word_prob_question_val.append(prob_i)\n","\n","\n","    #prob_ques_val = [word_distribution_ques.get(word) for word in flat_ques_tokens_val]\n","\n","\n","    #print(f'Sample documents unigrams:',' | '.join(sample_unigram_docs))\n","    #print(f'Sample questions unigrams:',' | '.join(sample_unigram_ques))\n","    print('Perplexity for documents:', np.round(gmean([1/y for y in word_prob_documents_val])))\n","    print('Perplexity for questions:', np.round(gmean([1/y for y in word_prob_question_val])))\n","    print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGcLy1dmlf2z","executionInfo":{"status":"ok","timestamp":1698497581393,"user_tz":-120,"elapsed":5931,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"}},"outputId":"f148081e-a626-481f-ee20-e69724c6fb3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##################################################\n","arabic\n","##################################################\n","Total Words Document Test arabic 155827\n","Total Vocabulary Document Test arabic 37483\n","Total Words Questions Test arabic 14554\n","Total Vocabulary Questions Test arabic 2274\n","Total Words Document Training arabic 2706703\n","Total Vocabulary Document Training arabic 225191\n","Total Words Question Training arabic 230005\n","Total Vocabulary Question Training arabic 16361\n","Perplexity for documents: 8509.0\n","Perplexity for questions: 538.0\n","--------------------------------------------------\n","##################################################\n","bengali\n","##################################################\n","Total Words Document Test bengali 22687\n","Total Vocabulary Document Test bengali 6727\n","Total Words Questions Test bengali 2368\n","Total Vocabulary Questions Test bengali 438\n","Total Words Document Training bengali 483302\n","Total Vocabulary Document Training bengali 50094\n","Total Words Question Training bengali 48674\n","Total Vocabulary Question Training bengali 3759\n","Perplexity for documents: 3822.0\n","Perplexity for questions: 444.0\n","--------------------------------------------------\n","##################################################\n","indonesian\n","##################################################\n","Total Words Document Test indonesian 103005\n","Total Vocabulary Document Test indonesian 18217\n","Total Words Questions Test indonesian 10088\n","Total Vocabulary Questions Test indonesian 1397\n","Total Words Document Training indonesian 1023833\n","Total Vocabulary Document Training indonesian 72405\n","Total Words Question Training indonesian 93069\n","Total Vocabulary Question Training indonesian 5774\n","Perplexity for documents: 3033.0\n","Perplexity for questions: 347.0\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### 2.3. Bigram"],"metadata":{"id":"FfWubJD7iIYZ"}},{"cell_type":"code","source":["# Bigram Counting Function\n","def function_count_bigrams(list_words=[], unigram_vocab={}):\n","    bigram_count = {}\n","    for i in range(1, len(list_words)):\n","        # Replace words not in our unigram training model with '[OOV]'\n","        # Different from replace_OOV() because it counts bigrams\n","        w1 = list_words[i-1] if list_words[i-1] in unigram_vocab.keys() else '[OOV]'\n","        w2 = list_words[i] if list_words[i] in unigram_vocab.keys() else '[OOV]'\n","        bigram_i = (w1, w2)\n","        if bigram_i in bigram_count.keys():\n","            bigram_count[bigram_i] += 1\n","        else:\n","            bigram_count[bigram_i] = 1\n","    return bigram_count\n","\n","# Bigram Model Function with Laplace Smoothing\n","def model_bigram(dict_bigrams={}, dict_unigrams={}, k=1):\n","    bigram_probabilities = {}\n","    V = len(dict_unigrams)  # Vocabulary size\n","    for bigram, count in dict_bigrams.items():\n","        word1 = bigram[0]\n","        bigram_probabilities[bigram] = (count + k) / (dict_unigrams[word1] + k*V)\n","    return bigram_probabilities"],"metadata":{"id":"1OUTaqTsMzeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["languages = ['arabic', 'bengali', 'indonesian']\n","\n","for language in languages:\n","    # Training\n","    sentences_doc_train = df_train_filter[df_train_filter['language'] == language]['tokens_document_endsentence']\n","    sentences_ques_train = df_train_filter[df_train_filter['language'] == language]['tokens_question_endsentence']\n","\n","    flat_doc_tokens_train = [item for sublist in sentences_doc_train for item in sublist]\n","    flat_ques_tokens_train = [item for sublist in sentences_ques_train for item in sublist]\n","\n","    counts_documents_train = function_count_words(flat_doc_tokens_train)\n","    counts_questions_train = function_count_words(flat_ques_tokens_train)\n","\n","    updated_vocab_docs_train = replace_OOV(counts_documents_train)\n","    updated_vocab_ques_train = replace_OOV(counts_questions_train)\n","\n","    bigram_counts_docs = function_count_bigrams(flat_doc_tokens_train, updated_vocab_docs_train)\n","    bigram_counts_ques = function_count_bigrams(flat_ques_tokens_train, updated_vocab_ques_train)\n","\n","    bigram_distribution_docs = model_bigram(bigram_counts_docs, updated_vocab_docs_train)\n","    bigram_distribution_ques = model_bigram(bigram_counts_ques, updated_vocab_ques_train)\n","\n","    # Example of a sample bigram model\n","    sample_doc_bigram_size_10 = []\n","    sample_ques_bigram_size_10 = []\n","    for i in range(0, 10):\n","       # Normalize the probabilities\n","        doc_probs = np.array(list(bigram_distribution_docs.values()))\n","        doc_probs /= doc_probs.sum()\n","\n","        ques_probs = np.array(list(bigram_distribution_ques.values()))\n","        ques_probs /= ques_probs.sum()\n","\n","        sample_doc_bigram_i = np.random.choice([' '.join(x) for x in bigram_distribution_docs.keys()],\n","                           p=doc_probs)\n","        sample_ques_bigram_i = np.random.choice([' '.join(x) for x in bigram_distribution_ques.keys()],\n","                            p=ques_probs)\n","\n","        sample_doc_bigram_size_10.append(sample_doc_bigram_i)\n","        sample_ques_bigram_size_10.append(sample_ques_bigram_i)\n","\n","\n","\n","    # Validation\n","    sentences_doc_val = df_val_filter[df_val_filter['language'] == language]['tokens_document_endsentence']\n","    sentences_ques_val = df_val_filter[df_val_filter['language'] == language]['tokens_question_endsentence']\n","\n","    flat_doc_tokens_val = [item for sublist in sentences_doc_val for item in sublist]\n","    flat_ques_tokens_val = [item for sublist in sentences_ques_val for item in sublist]\n","\n","    # Convert unseen unigrams to [OOV]\n","    updated_vocab_docs_val = [word if word in updated_vocab_docs_train.keys() else '[OOV]' for word in flat_doc_tokens_val]\n","    updated_vocab_ques_val = [word if word in updated_vocab_ques_train.keys() else '[OOV]' for word in flat_ques_tokens_val]\n","\n","    # Convert unseen bigrams to use [OOV] or take unigram probabilities if one of the words in the bigram is unseen\n","    log_prob_docs_val = []\n","    for i in range(1, len(updated_vocab_docs_val)):\n","        bigram = (updated_vocab_docs_val[i-1], updated_vocab_docs_val[i])\n","        if bigram in bigram_distribution_docs:\n","            log_prob_docs_val.append(np.log(bigram_distribution_docs[bigram]))\n","        else:\n","            # If bigram not seen, fall back to unigram probability of the second word\n","            log_prob_docs_val.append(np.log(updated_vocab_docs_train.get(updated_vocab_docs_val[i], updated_vocab_docs_train['[OOV]'])))\n","\n","    log_prob_ques_val = []\n","    for i in range(1, len(updated_vocab_ques_val)):\n","        bigram = (updated_vocab_ques_val[i-1], updated_vocab_ques_val[i])\n","        if bigram in bigram_distribution_ques:\n","            log_prob_ques_val.append(np.log(bigram_distribution_ques[bigram]))\n","        else:\n","            # If bigram not seen, fall back to unigram probability of the second word\n","            log_prob_ques_val.append(np.log(updated_vocab_ques_train.get(updated_vocab_ques_val[i], updated_vocab_ques_train['[OOV]'])))\n","\n","    # Compute perplexity based on the log bigram probabilities\n","    perplexity_docs_val = np.exp(-np.mean(log_prob_docs_val))\n","    perplexity_ques_val = np.exp(-np.mean(log_prob_ques_val))\n","\n","    print(\"#\"*50)\n","    print(language)\n","    print(\"#\"*50)\n","    print(f'Sample documents bigrams:',' | '.join(sample_doc_bigram_size_10))\n","    print(f'Sample questions bigrams:',' | '.join(sample_ques_bigram_size_10))\n","    print('Perplexity for documents:', np.round(perplexity_docs_val, 2))\n","    print('Perplexity for questions:', np.round(perplexity_ques_val, 2))\n","\n","    print(\"-\"*50)"],"metadata":{"id":"Nh6HK9iSi5KF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698496152706,"user_tz":-120,"elapsed":26126,"user":{"displayName":"Alan Miguel Forero Sanabria","userId":"15051656969276578416"}},"outputId":"61060ab0-c612-4e59-86f7-207cc1af80dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##################################################\n","arabic\n","##################################################\n","Sample documents bigrams: من أي | كما يلعب | ولد نيفيل | و شكلوا | جميع الأطفال | ماري كوري | سم عرضاً | كوسوفو . | منذ ذلك | النظير 235U\n","Sample questions bigrams: ظهرت الديانة | هو مبتكر | ماري [OOV] | [EOS] [SOS] | الانجليزي ؟ | جنسية العالمة | كانت مذبحة | عمر وحيد | البحر؟ [EOS] | العزيز آل\n","Perplexity for documents: 64.08\n","Perplexity for questions: 11.19\n","--------------------------------------------------\n","##################################################\n","bengali\n","##################################################\n","Sample documents bigrams: অধ্যাপক নিযুক্ত | ফলে পশ্চিম | ছিলেন একজন | [OOV] ভূমিকায় | ( এপ্রিল | , [OOV] | ছাড়া অন্য | তাঁদের [OOV] | অত্যধিক [OOV] | একটি সর্বেশ্বরবাদী\n","Sample questions bigrams: রচনা করেন | আল [OOV] | ভারতের পশ্চিমবঙ্গ | কী ? | স্ত্রীর নাম | জে. কে. | [EOS] [SOS] | অস্ট্রেলীয় আন্তর্জাতিক | মায়ের নাম | দেশ জয়ী\n","Perplexity for documents: 34.67\n","Perplexity for questions: 8.98\n","--------------------------------------------------\n","##################################################\n","indonesian\n","##################################################\n","Sample documents bigrams: menyebut mereka | bagi aliran | maka komunikasi | Jerman [OOV] | seperti Kera | kendali Eropa | diberi ganti | mundur di | di 1985 | ciri bangsa\n","Sample questions bigrams: [EOS] [SOS] | Apa itu | konsep Hak | yang digunakan | [EOS] [SOS] | nama ilmiah | dari DNA | Theater ? | Toyotomi [OOV] | dirilis ?\n","Perplexity for documents: 38.86\n","Perplexity for questions: 8.41\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["Updated last cell for Bigram"],"metadata":{"id":"Fw2ZhUwDtBQ-"}},{"cell_type":"code","source":["k =1\n","languages = ['arabic', 'bengali', 'indonesian']\n","\n","for language in languages:\n","    # Training\n","    sentences_doc_train = df_train_filter[df_train_filter['language'] == language]['tokens_document_endsentence']\n","    sentences_ques_train = df_train_filter[df_train_filter['language'] == language]['tokens_question_endsentence']\n","\n","    flat_doc_tokens_train = [item for sublist in sentences_doc_train for item in sublist]\n","    flat_ques_tokens_train = [item for sublist in sentences_ques_train for item in sublist]\n","\n","    counts_documents_train = function_count_words(flat_doc_tokens_train)\n","    counts_questions_train = function_count_words(flat_ques_tokens_train)\n","\n","    updated_vocab_docs_train = replace_OOV(counts_documents_train)\n","    updated_vocab_ques_train = replace_OOV(counts_questions_train)\n","\n","    bigram_counts_docs = function_count_bigrams(flat_doc_tokens_train, updated_vocab_docs_train)\n","    bigram_counts_ques = function_count_bigrams(flat_ques_tokens_train, updated_vocab_ques_train)\n","\n","    bigram_distribution_docs = model_bigram(bigram_counts_docs, updated_vocab_docs_train)\n","    bigram_distribution_ques = model_bigram(bigram_counts_ques, updated_vocab_ques_train)\n","\n","    # Example of a sample bigram model\n","    sample_doc_bigram_size_10 = []\n","    sample_ques_bigram_size_10 = []\n","    for i in range(0, 10):\n","       # Normalize the probabilities\n","        doc_probs = np.array(list(bigram_distribution_docs.values()))\n","        doc_probs /= doc_probs.sum()\n","\n","        ques_probs = np.array(list(bigram_distribution_ques.values()))\n","        ques_probs /= ques_probs.sum()\n","\n","        sample_doc_bigram_i = np.random.choice([' '.join(x) for x in bigram_distribution_docs.keys()],\n","                           p=doc_probs)\n","        sample_ques_bigram_i = np.random.choice([' '.join(x) for x in bigram_distribution_ques.keys()],\n","                            p=ques_probs)\n","\n","        sample_doc_bigram_size_10.append(sample_doc_bigram_i)\n","        sample_ques_bigram_size_10.append(sample_ques_bigram_i)\n","\n","\n","\n","    # Validation\n","    sentences_doc_val = df_val_filter[df_val_filter['language'] == language]['tokens_document_endsentence']\n","    sentences_ques_val = df_val_filter[df_val_filter['language'] == language]['tokens_question_endsentence']\n","\n","    flat_doc_tokens_val = [item for sublist in sentences_doc_val for item in sublist]\n","    flat_ques_tokens_val = [item for sublist in sentences_ques_val for item in sublist]\n","\n","    # Convert unseen unigrams to [OOV]\n","    updated_vocab_docs_val = [word if word in updated_vocab_docs_train.keys() else '[OOV]' for word in flat_doc_tokens_val]\n","    updated_vocab_ques_val = [word if word in updated_vocab_ques_train.keys() else '[OOV]' for word in flat_ques_tokens_val]\n","\n","    # Define the vocabulary size including the OOV token\n","    V_docs = len(updated_vocab_docs_train) + 1  # Adding 1 for the OOV token\n","    V_ques = len(updated_vocab_ques_train) + 1\n","\n","    # Convert unseen bigrams to use [OOV] or take unigram probabilities if one of the words in the bigram is unseen\n","    log_prob_docs_val = [np.log(updated_vocab_docs_train.get(flat_doc_tokens_val[0], updated_vocab_docs_train['[OOV]']) + k) - np.log(sum(updated_vocab_docs_train.values()) + k*V_docs)]\n","\n","    # Then proceed with the rest of the tokens\n","    for i in range(1, len(flat_doc_tokens_val)):\n","        bigram = (updated_vocab_docs_val[i-1], updated_vocab_docs_val[i])\n","        if bigram in bigram_distribution_docs:\n","           log_prob_docs_val.append(np.log(bigram_distribution_docs[bigram]))\n","        else:\n","           # If bigram not seen, fall back to Laplace-smoothed unigram probability of the second word\n","           log_prob_docs_val.append(np.log(updated_vocab_docs_train.get(updated_vocab_docs_val[i], k) + k) - np.log(sum(updated_vocab_docs_train.values()) + k*V_docs))\n","\n","    # Do the same for ques\n","    log_prob_ques_val = [np.log(updated_vocab_ques_train.get(flat_ques_tokens_val[0], updated_vocab_ques_train['[OOV]']) + k) - np.log(sum(updated_vocab_ques_train.values()) + k*V_ques)]\n","\n","    # Then proceed with the rest of the tokens\n","    for i in range(1, len(flat_ques_tokens_val)):\n","        bigram = (updated_vocab_ques_val[i-1], updated_vocab_ques_val[i])\n","        if bigram in bigram_distribution_ques:\n","           log_prob_ques_val.append(np.log(bigram_distribution_ques[bigram]))\n","        else:\n","           # If bigram not seen, fall back to Laplace-smoothed unigram probability of the second word\n","           log_prob_ques_val.append(np.log(updated_vocab_ques_train.get(updated_vocab_ques_val[i], k) + k) - np.log(sum(updated_vocab_ques_train.values()) + k*V_ques))\n","\n","    # Compute perplexity based on the log bigram probabilities\n","    perplexity_docs_val = np.exp(-sum(log_prob_docs_val) / (len(flat_doc_tokens_val) - 1))\n","    perplexity_ques_val = np.exp(-sum(log_prob_ques_val) / (len(flat_ques_tokens_val) - 1))\n","\n","    print(\"#\"*50)\n","    print(language)\n","    print(\"#\"*50)\n","    print(f'Sample documents bigrams:',' | '.join(sample_doc_bigram_size_10))\n","    print(f'Sample questions bigrams:',' | '.join(sample_ques_bigram_size_10))\n","    print('Perplexity for documents:', np.round(perplexity_docs_val, 2))\n","    print('Perplexity for questions:', np.round(perplexity_ques_val, 2))\n","\n","    print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxuLCVLSkvh3","executionInfo":{"status":"ok","timestamp":1698922944980,"user_tz":-60,"elapsed":42854,"user":{"displayName":"Alex YE","userId":"10586116328910308711"}},"outputId":"6287de1e-4400-4b0e-cde4-f0fdea64b68f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##################################################\n","arabic\n","##################################################\n","Sample documents bigrams: هيغز ، | تحليل لتاريخ | من بدء | إبراهيموفيتش عدة | الجزيرة الأيبيرية، | المعدن الملكي | اختبار نسبة | العليا للإمبراطورية | عام 2005 | يصل إلى\n","Sample questions bigrams: [EOS] [SOS] | عاصمة ألمانيا | في العراق؟ | الهيثم؟ [EOS] | هي الالعاب | استلم محمود | [OOV] [EOS] | العشرين ؟ | [EOS] [SOS] | [OOV] بيكاسو؟\n","Perplexity for documents: 2374.59\n","Perplexity for questions: 30.45\n","--------------------------------------------------\n","##################################################\n","bengali\n","##################################################\n","Sample documents bigrams: গ্রামে বেড়ে | শহরের জনসংখ্যা | নির্বাচিত হয়েছিলেন | - [OOV] | খলিফা রাসুলুল্লাহ | পৌত্র [OOV] | চাকরি নাই | আরএনএ ) | মার্চ [OOV] | চলচ্চিত্রে সুপারহিরো\n","Sample questions bigrams: ? [EOS] | কোন [OOV] | তৈরী হয় | অন্তর্গত ? | ? [EOS] | প্রথম [OOV] | করেছিলেন ? | মোট কয়টি | কাব্যগ্রন্থটি কত | [SOS] আলী\n","Perplexity for documents: 699.77\n","Perplexity for questions: 29.08\n","--------------------------------------------------\n","##################################################\n","indonesian\n","##################################################\n","Sample documents bigrams: 8 ] | ia disebut | ilmu pengetahuan | untuk mengontrol | membandingkan garis | daerah ini | Pusat Penelitian | Kaisar Yang | , [OOV] | Marvel Comics\n","Sample questions bigrams: mulai dikembangkan | peran [OOV] | berapakah luas | asia [OOV] | Ada berapa | pembakaran dalam | Buonarroti ? | newton dilahirkan | Kristen ? | ibukota Australia\n","Perplexity for documents: 768.63\n","Perplexity for questions: 18.49\n","--------------------------------------------------\n"]}]}]}